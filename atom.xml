<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>00&#39;s Adventure</title>
  
  <subtitle>Why join the navy if you can be a pirate</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://uegeek.com/"/>
  <updated>2017-12-31T09:30:20.150Z</updated>
  <id>http://uegeek.com/</id>
  
  <author>
    <name>kidult00</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>小哉问：年终总结写什么？</title>
    <link href="http://uegeek.com/171226MyYear2017.html"/>
    <id>http://uegeek.com/171226MyYear2017.html</id>
    <published>2017-12-29T11:28:42.000Z</published>
    <updated>2017-12-31T09:30:20.150Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/2017sumTitle.jpg" alt=""></p><a id="more"></a> <p>马上要跨入本世纪第 19 个年头了（可怕不可怕……），大家都在忙着写总结和计划。</p><p>年终总结写什么，取决于想得到什么。</p><p>如果想记录一年都做了什么，那很简单，按时间记账就行。如果想盘点一年的得失，就需要费一点心力，思考哪些事情比较重要，自己从中获得了什么、失去了什么。</p><p>如果想给自己一个交代，总结过去就好。</p><p>如果想许自己一个未来，那值得再琢磨琢磨。</p><p>一天有一天的所得，一辈子有一辈子的教训。越是大时间周期的回顾，提取的信息应该越凝练。人能清醒地写年终总结的机会真的不多（也就二三十来次？）年末提供了一个强制的时间点去做盘点，以便搞清楚这一年有哪些新收获，最希望在来年谨记？有哪些切肤之痛，希望未来不要再经历？有哪b些或主动或被动的改变，希望来年继续？</p><p>一年一次的总结，也可以看成是一次内存整理：<strong>「卸载」那些无用、低效的想法和习惯，「加载」来年需要的认知和能力</strong>。</p><p>今年是出乎意料的一年，以没有料想过的方式，学习到一些被忽视已久的知识。虽然作品寥寥，倒是更懂自己了。</p><p>来到 2018 的门口，先放下背包，倒出这一年中收集的种种，仔细考虑哪些要丢弃，哪些需要重视起来，哪些要且行且珍惜。</p><h3 id="需要卸载"><a href="#需要卸载" class="headerlink" title="需要卸载"></a>需要卸载</h3><ul><li>会带来巨大认知失调的惯性</li><li>让别人的目标凌驾于自己的目标之上</li><li>自我剥夺价值感</li><li>陈旧的人设</li></ul><h3 id="需要保持"><a href="#需要保持" class="headerlink" title="需要保持"></a>需要保持</h3><ul><li>简单的生活方式</li><li>反碎片化</li><li>从知识源头获取信息（比如，跟踪人而不是五手信息）</li><li>以问题驱动思路，以试验驱动行动</li></ul><h3 id="需要加载"><a href="#需要加载" class="headerlink" title="需要加载"></a>需要加载</h3><p><strong>1. 进入新环境、新领域、新角色时，需要清内存，重建数据库</strong></p><p><strong>2. 正确看待人性的复杂</strong></p><ul><li>尤其不要低估人与人之间的差异</li><li>提升快速识别人的判断力，以及其他直觉</li><li>观察互动模式如何形成</li><li>设定关系的止损点</li></ul><p><strong>3. 爱自己</strong></p><ul><li>通过情绪快速识别问题</li><li>划定灵活而坚定的个人边界</li><li>优先处理自己的核心矛盾，保护内在动机</li></ul><p><strong>4. 建立「初心」的快捷方式，多拷问目标</strong></p><p><strong>5. 减少 90% 主流信息输入方式，在最优信息上增加十倍投入</strong></p><p><strong>6. 以身份和项目为导向，聚焦和持续输出</strong></p><hr><p>祝大家新年快乐！每天都有新收获</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/2017sumTitle.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="2017" scheme="http://uegeek.com/tags/2017/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：用 python 实现梯度下降的算法</title>
    <link href="http://uegeek.com/171226DLN7-GradientDescentinPython.html"/>
    <id>http://uegeek.com/171226DLN7-GradientDescentinPython.html</id>
    <published>2017-12-26T01:28:42.000Z</published>
    <updated>2017-12-27T01:40:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p>回顾上回讲的梯度下降算法，想实现梯度下降，需要不断更新 w：</p><p>$$ \Delta w_{ij} = \eta \delta_j x_i $$</p><p>具体步骤如下：</p><ul><li>初始化权重变化率为 0 ：$\Delta w_i = 0$</li><li>对训练集中的每一个数据：<ul><li>做正前传播计算：$\hat y=f(\sum_iw_ix_i)$</li><li>计算输出单元的 error term：$\delta=(y-\hat y) * f’(\sum_iw_ix_i)$</li><li>更新权重变化率：$\Delta w_i= \Delta w_i + \delta x_i$</li></ul></li><li>更新权重 $w_i = w_i + \eta \Delta w_i /m$</li><li>重复 e 次训练 epochs</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li>初始化权重变化率为 0</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del_w = np.zeros(weights.shape)</span><br></pre></td></tr></table></figure><ol><li>正向传播计算</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = sigmoid(np.dot(x, weights))</span><br></pre></td></tr></table></figure><ol><li>计算输出单元的 error term</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">error = y - output</span><br><span class="line">error_term = error * output * (<span class="number">1</span>-output)</span><br></pre></td></tr></table></figure><ol><li>更新权重变化率</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del_w += error_term * x</span><br></pre></td></tr></table></figure><ol><li>更新权重</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights += learnrate * del_w / n_records</span><br></pre></td></tr></table></figure><ol><li>重复 epochs</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    del_w = np.zeros(weights.shape)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(features.values, targets):</span><br><span class="line">        <span class="comment"># Loop through all records, x is the input, y is the target</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the output</span></span><br><span class="line">        output = sigmoid(np.dot(x, weights))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the error</span></span><br><span class="line">        error = y - output</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the error term</span></span><br><span class="line">        error_term = error * output * (<span class="number">1</span>-output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the change in weights for this sample</span></span><br><span class="line">        <span class="comment"># and add it to the total weight change</span></span><br><span class="line">        del_w += error_term * x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using the learning rate and the average change in weights</span></span><br><span class="line">    weights += learnrate * del_w / n_records</span><br></pre></td></tr></table></figure><p>完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> data_prep <span class="keyword">import</span> features, targets, features_test, targets_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the sigmoid function for activations</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reserve seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">n_records, n_features = features.shape</span><br><span class="line">last_loss = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize weights</span></span><br><span class="line">weights = np.random.normal(scale=<span class="number">1</span> / n_features**<span class="number">.5</span>, size=n_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neural Network hyperparameters</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">learnrate = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    del_w = np.zeros(weights.shape)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(features.values, targets):</span><br><span class="line">        <span class="comment"># Loop through all records, x is the input, y is the target</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the output</span></span><br><span class="line">        output = sigmoid(np.dot(x, weights))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the error</span></span><br><span class="line">        error = y - output</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the error term</span></span><br><span class="line">        error_term = error * output * (<span class="number">1</span>-output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the change in weights for this sample</span></span><br><span class="line">        <span class="comment"># and add it to the total weight change</span></span><br><span class="line">        del_w += error_term * x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using the learning rate and the average change in weights</span></span><br><span class="line">    weights += learnrate * del_w / n_records</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Printing out the mean square error on the training set</span></span><br><span class="line">    <span class="keyword">if</span> e % (epochs / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">        out = sigmoid(np.dot(features, weights))</span><br><span class="line">        loss = np.mean((out - targets) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> last_loss <span class="keyword">and</span> last_loss &lt; loss:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss, <span class="string">"  WARNING - Loss Increasing"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss)</span><br><span class="line">        last_loss = loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy on test data</span></span><br><span class="line">test_out = sigmoid(np.dot(features_test, weights))</span><br><span class="line">predictions = test_out &gt; <span class="number">0.5</span></span><br><span class="line">accuracy = np.mean(predictions == targets_test)</span><br><span class="line">print(<span class="string">"Prediction accuracy: &#123;:.3f&#125;"</span>.format(accuracy))</span><br></pre></td></tr></table></figure><h3 id="00-的-DeepLearning-笔记"><a href="#00-的-DeepLearning-笔记" class="headerlink" title="00 的 DeepLearning 笔记"></a>00 的 DeepLearning 笔记</h3><ul><li><a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">DL笔记：机器学习和深度学习的区别</a></li><li><a href="http://www.uegeek.com/171209DLN2-NeuralNetworks.html" target="_blank" rel="noopener">DL笔记：Neural Networks 神经网络</a></li><li><a href="http://www.uegeek.com/171213DLN3-LinearRegression.html" target="_blank" rel="noopener">DL笔记：Linear regression 线性回归</a></li><li><a href="http://www.uegeek.com/171218DLN4-ActivationFunction.html" target="_blank" rel="noopener">DL笔记：Activation Function 激活函数</a></li><li><a href="http://www.uegeek.com/171220DLN5-CostFunction.html" target="_blank" rel="noopener">DL笔记：Cost Function 损失函数</a></li><li><a href="http://www.uegeek.com/171222DLN6-GradientDescent.html" target="_blank" rel="noopener">DL笔记：Gradient Descent 梯度下降</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：梯度下降 Gradient Descent</title>
    <link href="http://uegeek.com/171222DLN6-GradientDescent.html"/>
    <id>http://uegeek.com/171222DLN6-GradientDescent.html</id>
    <published>2017-12-22T01:28:24.000Z</published>
    <updated>2017-12-31T09:09:06.079Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p><code>阿扣</code>：上一次我们了解了<a href="http://www.uegeek.com/171220DLN5-CostFunction.html" target="_blank" rel="noopener">损失函数</a>。为了找到使损失函数（比如用 SSE 计算）最小的 w (权重) 和 b (偏置项)，我们需要先了解一个重要的方法：梯度下降。</p><p><code>阿特</code>：听起来像坐滑滑梯~</p><p><code>阿扣</code>：是有那么点意思。</p><p><code>阿扣</code>：想象一下，我们对网络中的一些权重做了很小的改变，这些变化会让输出也有相应很小的变化：</p><p><img src="http://neuralnetworksanddeeplearning.com/images/tikz8.png" alt=""></p><p>via <a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener">Neural networks and deep learning - chapter 1</a></p><p>然后拿这些微小的变化，跟目标值对比，看看误差是变大还是变小了，然后不断调整权重值，最终找到最合适的 w 和 b。</p><p><code>阿特</code>：那要怎么找到这些值呢？</p><p><code>阿扣</code>：下面有请「梯度下降」 Gradient Descent。</p><p><code>阿特</code>：终于能坐滑滑梯了……</p><p><code>阿扣</code>：坐这个滑滑梯可能有点晕 😄 。我先给你打个比方。想象一下，你在一个山峰的山顶，想用最快的速度到达山脚。</p><p><code>阿特</code>：坐缆车可以吗？</p><p><code>阿扣</code>：缆车，不存在的……只能靠走的。要往哪边下山呢？我们会选一个看起来「下降」最快的路径：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i1.png" alt=""></p><p>朝这个方向走一段后，我们再看下一步往哪个方向走，「下降」最快。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i2.png" alt=""></p><p>一直重复这个过程，就能最快的速度下到山脚。</p><p><code>阿特</code>：是这么个道理。</p><p><code>阿扣</code>：这个方法，就是「梯度下降」，在机器学习中很常见。所谓「梯度」，其实是指「变化率」或者「坡度 slope」，就是多变量函数的导数。</p><p><code>阿特</code>：导数？！你说的是微积分里面那个导数吗？ …… 瑟瑟发抖.gif</p><p><code>阿扣</code>：别紧张，先听我讲，回忆回忆。</p><p><code>阿特</code>：好吧。</p><p><code>阿扣</code>：你还记得怎么表示函数 f(x) 的导数吧？很简单，就是 f’(x) 。</p><p><code>阿特</code>：嗯嗯，记得。</p><p><code>阿扣</code>：所谓「梯度」，其实就是函数在某一点上的变化率，根据微分的知识，变化率可以通过这一点的切线求得，而切线其实就是函数的导数：f’(x)。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/derivative-example.png" alt=""></p><p>来，跟我念一遍：求梯度 = 求变化率 = 求导数</p><p><code>阿特</code>：求梯度 = 求变化率 = 求导数 （假装自己听懂了）</p><p><code>阿扣</code>：了解了「梯度」，然后我们来看看「下降」又是怎么回事。</p><p>切线代表函数在某个点的变化率。在上面这个图中，x = 2 位置上的切线，斜率是 &gt; 1 的。说明如果继续往 x = 2 的右边滑去，在曲线上的值就会变大。比如当 x = 3 时，y = 9。</p><p>但是我们想要到曲线最低的地方去，因为那里可以让误差（也就是 cost ）最小。所以，应该沿着梯度「相反」的方向滑动，也就朝着是 x = 2 的左边滑去。这就是「下降」的含义。</p><p><code>阿特</code>：沿着「上山」最快的反方向走，就能最快「下山」。啊原来这么直白……</p><p><code>阿扣</code>：对呀，原理并不复杂的。</p><p>这个视频讲解了线性回归和梯度下降的关系，来看看吧！</p><div class="video-container"><iframe src="//www.youtube.com/embed/L5QBqYDNJn0" frameborder="0" allowfullscreen></iframe></div><p><a href="https://www.youtube.com/watch?time_continue=194&amp;v=L5QBqYDNJn0" target="_blank" rel="noopener">Linear Regression Answer - YouTube</a></p><p><code>阿特</code>：这个视频不错，讲得挺清楚的~</p><p><code>阿扣</code>：我们来复习一下。用一个函数 f(h) 表示 x 和 y 的关系。x 和 y 其实是已知的，它们来自真实的数据集。我们的目标是求出 w 和 b，使得计算出来的 $\hat y$ 最接近实际的 y 值。为了得到某种类型的 y 值（比如只有 0 和 1 两种输出），我们会使用类似 Sigmoid 这样的激活函数，对 f(h) 做一下转换。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/simpleNN.png" alt=""></p><p><code>阿特</code>：哦，我说怎么有点难理解呢。因为以前碰到 x 和 y，它们都是未知数，现在它们变成了已知数，真正的目标其实是求 w 和 b！</p><p><code>阿扣</code>：没错！这是深度学习算法中一个需要调整的认知。</p><p>怎么得到 w 和 b 呢？用损失函数。如果损失函数的值大，说明模型预测得不准。我们要找到让损失函数的值最小的 w 和 b。更具体说，我们要找到 w 的变化幅度 $\Delta w$，每次调整一小步，看看误差 E 是不是变小了。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Gradient%20Descent-i3.png" alt=""></p><p>为了求出 $\Delta w$，我们引入「误差项」$\delta$ ，它表示 <code>误差 * 激活函数的导数</code>。然后用「误差项」$\delta$ 乘上学习率 $\eta$ （用来调整梯度的大小），再乘上 x，就是每次应该调整的权重值 $\Delta w_{ij}$</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/WX20171127-154242@2x.png" alt=""></p><p><code>阿扣</code>：比如说，如果激活函数是 Sigmoid 函数。</p><p>$$ f(h)=\frac {1}{1 + e^{−h}}<br>\<br>f’(h)=f(h)(1−f(h))<br>\<br>\Delta w_{ij}=\eta<em>(y_j-\hat y_j)</em>f(h)<em>(1−f(h))</em>x_i $$</p><p>**<br>…… 咦？人呢？</p><p>喂！别跑，还有好几个知识点没讲呢！……</p><h3 id="补充：求多个变量的偏导数"><a href="#补充：求多个变量的偏导数" class="headerlink" title="补充：求多个变量的偏导数"></a>补充：求多个变量的偏导数</h3><p>如果只有一个未知数，求梯度只需要计算导数。如果有多个变量，求梯度就需要计算偏导数。偏导数其实并不复杂，只需要掌握链式求导法则，就能进行大部分的计算。</p><p>$$ \frac{\partial}{\partial w} p(q(w)) = \frac{\partial p}{\partial q}\frac{\partial q}{\partial w} $$</p><p>比如，损失函数 C</p><p>$$ C = \sum(wx + b - y)^2 = \sum((wx + b)^2 + y^2 - 2y(wx + b)) = \sum(x^2w^2 + b^2 + 2xwb + y^2 - 2xyw - 2yb) $$</p><p>对 w 求偏导</p><p>$$ \frac{\partial C}{\partial w} = \frac{1}{N} \sum(wx + b - y)x $$</p><p>对 b 求偏导</p><p>$$ \frac{\partial C}{\partial b} = \frac{1}{N} \sum(wx + b - y) $$</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101" target="_blank" rel="noopener">Deep Learning Nanodegree | Udacity</a></li><li><a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">Neural Networks and Deep Learning | Coursera</a></li><li><a href="https://classroom.udacity.com/nanodegrees/nd101-cn/parts/ba124b66-b7f7-43ab-bc89-a390adb57f92/modules/2afd43e6-f4ce-4849-bde6-49d7164da71b/lessons/dc37fa92-75fd-4d41-b23e-9659dde80866/concepts/7d480208-0453-4457-97c3-56c720c23a89" target="_blank" rel="noopener">Gradient Descent with Squared Errors</a></li><li><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient" target="_blank" rel="noopener">Gradient (video) | Khan Academy</a></li><li><a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li></ul><h3 id="00-的-DeepLearning-笔记"><a href="#00-的-DeepLearning-笔记" class="headerlink" title="00 的 DeepLearning 笔记"></a>00 的 DeepLearning 笔记</h3><ul><li><a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">DL笔记：机器学习和深度学习的区别</a></li><li><a href="http://www.uegeek.com/171209DLN2-NeuralNetworks.html" target="_blank" rel="noopener">DL笔记：Neural Networks 神经网络</a></li><li><a href="http://www.uegeek.com/171213DLN3-LinearRegression.html" target="_blank" rel="noopener">DL笔记：Linear regression 线性回归</a></li><li><a href="http://www.uegeek.com/171218DLN4-ActivationFunction.html" target="_blank" rel="noopener">DL笔记：Activation Function 激活函数</a></li><li><a href="http://www.uegeek.com/171220DLN5-CostFunction.html" target="_blank" rel="noopener">DL笔记：Cost Function 损失函数</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：Cost function 损失函数</title>
    <link href="http://uegeek.com/171220DLN5-CostFunction.html"/>
    <id>http://uegeek.com/171220DLN5-CostFunction.html</id>
    <published>2017-12-20T01:31:16.000Z</published>
    <updated>2017-12-27T01:38:19.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p><code>阿扣</code>：阿特，还记得训练神经网络的目标其实是什么吗？</p><p><code>阿特</code>：我记得好像是要找出最合适的权重(weights)，使得输出结果尽可能接近真实值。</p><p><code>阿扣</code>：Hin 棒！你说的没错。说回到训练神经网络，我们需要在训练中及时了解训练效果如何，是不是朝着训练目标在一点点靠近。如果偏离目标，就说明训练模型可能在「犯错」，就要纠正过来。</p><p><code>阿特</code>：那怎么知道模型是不是在「犯错」呢？</p><p><code>阿扣</code>：我们会找一个度量标准。一个常见的度量方法是计算误差的平方和（SSE, sum of the squared errors）：</p><p>$$ E=\frac{1}{2}\sum_\mu\sum_j[y^\mu_j - f(\sum<em>i w</em>{ij}x^\mu_i)]^2 $$</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/latex_9521ee448af952b9e073b5d31974241c.png" alt=""></p><p><code>阿特</code>：你……欺负人 &gt;.&lt;</p><p><code>阿扣</code>：别着急，我们来拆解这一坨是个什么东西。先看看各个字母的含义：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/il_for_SSE-1.png" alt=""></p><p>这个等式里面，有三个求和项（就是这个翻转了 90° 的 M： $\sum$ ）。</p><p>最右边的求和项 $\sum<em>i w</em>{ij}x^\mu_i$ ，表示我们训练出来的权重 w 乘上输入值 x 得出的目标值 $\hat y$（也就是我们给数据打算的标签），然后用这些结果跟实际的数据中的 y 值做比较，看看偏差有多大。</p><p>现在你理解了最右边的求和项了吗？</p><p><code>阿特</code>：大概意思是我们从数据中预测出来的 y ？</p><p><code>阿扣</code>：没错，我们先把这一坨替换成 $\hat y$，简化一下公式：</p><p>$$<br>E=\frac{1}{2}\sum_\mu\sum_j[y^\mu_j - f(\sum<em>i w</em>{ij}x^\mu<em>i)]^2<br>\<br>\downarrow<br>\<br>E=\frac{1}{2}\sum</em>\mu\sum_j[y^\mu_j - \hat y_j]^2<br>$$</p><p><code>阿特</code>：世界清静多了~</p><p><code>阿扣</code>：我们再来看右边这个求和项。j 表示有 j 个隐层节点，把每个节点的误差平方 $[y^\mu_j - \hat y_j]$ 计算出来。现在只剩下最后一个求和项了，它表示把 u 个输出节点的误差加起来。这样就得到了总体误差。</p><h3 id="00-的-DeepLearning-笔记"><a href="#00-的-DeepLearning-笔记" class="headerlink" title="00 的 DeepLearning 笔记"></a>00 的 DeepLearning 笔记</h3><ul><li><a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">DL笔记：机器学习和深度学习的区别</a></li><li><a href="http://www.uegeek.com/171209DLN2-NeuralNetworks.html" target="_blank" rel="noopener">DL笔记：Neural Networks 神经网络</a></li><li><a href="http://www.uegeek.com/171213DLN3-LinearRegression.html" target="_blank" rel="noopener">DL笔记：Linear regression 线性回归</a></li><li><a href="http://www.uegeek.com/171218DLN4-ActivationFunction.html" target="_blank" rel="noopener">DL笔记：Activation Function 激活函数</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：Activation Function 激活函数</title>
    <link href="http://uegeek.com/171218DLN4-ActivationFunction.html"/>
    <id>http://uegeek.com/171218DLN4-ActivationFunction.html</id>
    <published>2017-12-18T11:20:51.000Z</published>
    <updated>2017-12-18T11:59:31.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p>回顾:</p><ul><li><a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">DL笔记：机器学习和深度学习的区别</a></li><li><a href="http://www.uegeek.com/171209DLN2-NeuralNetworks.html" target="_blank" rel="noopener">DL笔记：Neural Networks 神经网络</a></li><li><a href="http://www.uegeek.com/171213DLN3-LinearRegression.html" target="_blank" rel="noopener">DL笔记：Linear regression 线性回归</a></li></ul><p><code>阿扣</code>：阿特，今天我们来了解一下深度学习中的激活函数(Activation functions)。</p><p><code>阿特</code>：又是函数……为什么要了解这个哦……</p><p><code>阿扣</code>：在机器学习中，我们经常需要对输出结果打上「是」或「否」标签。比如对一张输入的图片，模型要判断图片里面有没有包含汪星人。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Col.DL.dog_detect.png" alt=""></p><p><a href="http://www.uegeek.com/171213DLN3-LinearRegression.html" target="_blank" rel="noopener">上一回我们提到的逻辑回归</a>，可以用来减少预测值和真实值之间的误差。</p><p><code>阿特</code>：那要怎么做呢？</p><p><code>阿扣</code>：我们来用符号描述一下问题：</p><ul><li>x：训练数据中的 input</li><li>y：训练数据中已经做好标记的 output</li><li>w：逻辑回归的 weights</li><li>b：逻辑回归的 bias</li><li>模型的输出：$\hat y = \sigma (wx + b)$</li></ul><p><code>阿特</code>：老朋友 wx + b</p><p><code>阿扣</code>：好眼力。它就是一个线性模型。别忘了，我们想让输出只包含两个值：是，否。一般我们会用 1 表示「是」，用 0 表示「否」。</p><p><code>阿特</code>：就是我给模型图片 A，它说「0」；给图片 B，它说「1」；……这样？</p><p><code>阿扣</code>：没错~ 所以我们把结果的输出全部转换成或 0 或 1 的值。激活函数就是用来帮助我们实现这种转化的。</p><p><img src="https://ml4a.github.io/images/figures/sigmoid.png" alt=""></p><p>上面我们用到的激活函数叫做 Sigmoid 函数。它帮我们做到了：</p><ul><li>如果输入值 z 是一个大的正数，函数的输出值为 1；</li><li>如果输入值 z 是一个大的负数，函数的输出值为 0；</li><li>如果输入值 z = 0，那么输出值是 0.5</li></ul><p><code>阿特</code>：也就是说，不论我给什么样的整数，最后都会返回 0 或 1 的结果？</p><p><code>阿扣</code>：没错！这样我们得到分类的结果，或 0 或 1。在深度学习中，这种<strong>把输出转化为我们想要的形式的函数</strong>，我们叫它「激活函数」：</p><blockquote><p>激活函数的主要作用是提供网络的非线性建模能力。如果没有激活函数，即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。加入激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。</p></blockquote><p>上图就是其中的一种激活函数：sigmoid 函数。</p><p><code>阿特</code>：这么说，激活函数不止一种？</p><p><code>阿扣</code>：对呀。下面我列了一些常用的激活函数，作为今天的补充资料吧。现在可能还看不到，先混个脸熟就好。</p><p><code>阿特</code>：好的先刷脸。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>$$ sigmoid(z)= \frac{1}{(1+e​^{−z})} $$</p><p>Sigmoid 函数取值范围为(0,1)，将一个实数映射到(0,1)的区间，可以用来做二分类。</p><p><img src="https://ml4a.github.io/images/figures/sigmoid.png" alt=""></p><p>Sigmoid 在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid 的导数最大值为0.25。这意味着用来进行反向传播时，返回网络的 error 将会在每一层收缩至少75％（梯度消失问题）。对于接近输入层的层，如果有很多层， weights 更新会很小。</p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>$$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}} $$</p><p>也称为双切正切函数，取值范围为[-1,1]。tanh 在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。</p><p><img src="http://mathworld.wolfram.com/images/interactive/TanhReal.gif" alt=""></p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>$$ReLU(z) = max(z,0)$$</p><p>ReLU (rectified linear units) 是现在较常用的激活函数。如果输入 &lt; 0，ReLU 输出 0；如果输入 &gt;0，输出等于输入值。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/relu.png" alt=""></p><p>ReLU 计算量小（不涉及除法），一部分神经元的输出为 0 造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</p><p>ReLU 的缺点是，梯度较大时，ReLU 单元可能大都是 0，产生大量无效的计算（特征屏蔽太多，导致模型无法学习到有效特征）。</p><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>$$ softmax(z) = \frac{e^z{<em>j}}{\sum^K</em>{k=1}e^z{_k}}$$</p><p>Softmax 函数将 K 维的实数向量压缩（映射）成另一个 K 维的实数向量，其中向量中的每个元素取值都介于(0，1)之间。<strong>常用于多分类问题</strong>。Softmax 把分数转换为概率分布，让正确的分类的概率接近 1，其他结果接近 0。相比 Sigmoid，它做了归一化处理。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/N_softmax.png" alt=""></p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101" target="_blank" rel="noopener">Deep Learning Nanodegree | Udacity</a></li><li><a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">Neural Networks and Deep Learning | Coursera</a></li><li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">Neural networks and deep learning</a></li><li><a href="http://cs231n.github.io/neural-networks-1/#nn" target="_blank" rel="noopener">Andrej Karpathy’s CS231n course</a></li><li><a href="http://blog.csdn.net/u014595019/article/details/52562159" target="_blank" rel="noopener">深度学习笔记(三)：激活函数和损失函数 - CSDN博客</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>大哉问03 - 什么是赚钱之道？更新你的个人商业模式</title>
    <link href="http://uegeek.com/171216HowToMakeMoney.html"/>
    <id>http://uegeek.com/171216HowToMakeMoney.html</id>
    <published>2017-12-16T11:03:13.000Z</published>
    <updated>2017-12-18T11:44:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/dzw03-title.png" alt=""></p><a id="more"></a> <p><a href="http://www.uegeek.com/171112-HowToAskGoodQuestion.html" target="_blank" rel="noopener">大哉问系列</a>讨论完<a href="http://www.uegeek.com/171126TimePerspective.html" target="_blank" rel="noopener">时间</a>和<a href="http://www.uegeek.com/171204HowToLoveYourself.html" target="_blank" rel="noopener">爱</a>，是时候来想想这个重大课题了。</p><p>这篇文章不是经验之谈，而是迟来的反省，也是下一步的行动大纲。</p><h2 id="财富迷思"><a href="#财富迷思" class="headerlink" title="财富迷思"></a>财富迷思</h2><h3 id="为什么要积累财富？"><a href="#为什么要积累财富？" class="headerlink" title="为什么要积累财富？"></a>为什么要积累财富？</h3><p>为了避免话题泛化，这里的「财富」，主要指物质财富、个人的资产。</p><p>有钱当然好。可是到底为什么好？这个问题我们真的有想过吗？到底是什么让我们有持续的动力去追求财富？这是破除迷思的第一个问题。因为赚钱的前提不是方法，而是<strong>欲望</strong>。</p><p>财富可能至少有三方面重要作用：</p><ul><li>提高生活质量：让自己和家人更自如地生活</li><li>缩短成长周期：换取时间，用于自我成长</li><li>获得更多可能性和选择权：贫穷真的真的会限制想象力</li></ul><p>钱越多越好，这毋庸置疑。但是赚钱的目标应该定在哪里合适呢？毕竟人生有限，只花在挣钱上也太枯燥了。大多数人的财务目标大概都会是：实现财务自由。</p><h3 id="什么是财务自由？"><a href="#什么是财务自由？" class="headerlink" title="什么是财务自由？"></a>什么是财务自由？</h3><p>财务自由有客观标准吗？500w？1000w？众说纷纭啊……</p><p>财务自由应该由具体的数值定义吗？我觉得很难。需要找一个定性但可操作的定义。</p><p>这个问题已经有非常多人思考和讨论过了，我比较认同这个目标：</p><blockquote><p>不需要被动出售自己的时间。</p></blockquote><p>换个说法，大概意思是时间都只花在完全由自己选择的事情上面。</p><h3 id="赚钱的瓶颈？"><a href="#赚钱的瓶颈？" class="headerlink" title="赚钱的瓶颈？"></a>赚钱的瓶颈？</h3><p>大部分人（包括我自己）是如何挣钱呢？——批量出售自己的时间（一般是以月为单位）给雇主，然后换取基本固定的薪水——无论干得怎样，薪水的浮动都不大。</p><p>这下子赚钱的天花板就出现了，因为每个人的时间都<strong>极其有限</strong>。</p><p>怎么突破瓶颈呢？一个方法是做薪酬更高的工作。于是我们去增强职场技能、提高工作效率、跳槽到更好的岗位等等，都是在想办法提升单位时间的报酬。</p><p>不过这样也容易掉入时空限制中的「空间」陷阱——「空间」局限意味着影响范围是有限的。一个人的劳动实际上只卖给了一个「空间」：就是这个公司、这个老板。无法 scalable，自然就会遇到瓶颈。（管理很多人是一种 scalable 的办法）</p><h3 id="思考财富的单位？"><a href="#思考财富的单位？" class="headerlink" title="思考财富的单位？"></a>思考财富的单位？</h3><p>想最大限度地保留自己的时间自主权和使用权，该怎么办？</p><p>可以考虑用空间换时间，而不是用时间换金钱。</p><p>怎样用空间换时间呢？——把每单位时间的产出，卖给更多人。</p><p>这里有两个变量：单位时间的产出，和更多人。</p><ul><li>增加单位时间的产出，可以让 1 小时可以写更多好文章、生产更好的产品、打造更完善的系统；也可以尽可能延长产出的寿命，畅销多年而不需要大规模维护。</li><li>同时出售给更多愿意购买的人</li></ul><p>这样，我们思考获得财富的单位，应该是<strong>有价值的结果</strong>，而不再是「月薪」，不再是一去不复返的时间。</p><h2 id="赚钱的三大途径"><a href="#赚钱的三大途径" class="headerlink" title="赚钱的三大途径"></a>赚钱的三大途径</h2><p>赚钱的方法何止千千万，但是<strong>从本质上讲，大概有三类：生产，服务和交易</strong>。</p><p>比如，制造可售卖的商品属于生产，搭建一个付费 APP 是生产；提供专属的理财咨询属于服务；炒币和投资房产属于交易。</p><p>那么上班属于哪一类？其实属于<strong>服务</strong>，因为肉身必需出现，而且离实际购买的消费者比较远，工作报酬受契约约束，而不是实际产出的市场价值本身的体现。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/wealth-title.jpg" alt=""></p><p>再深入一步，让我们想想这三大途径的特点：</p><table><thead><tr><th>方式</th><th>投入</th><th>产出</th><th>买单</th><th>瓶颈</th></tr></thead><tbody><tr><td>生产</td><td>原料，生产设备/技术/流程，销售</td><td>产品</td><td>顾客</td><td>生产率，获客成本</td></tr><tr><td>交易</td><td>资金/成本，决策信息，时间差</td><td>价格差</td><td>需方</td><td>时机，关键信息，资本</td></tr><tr><td>服务</td><td>时间，技能</td><td>交付物</td><td>雇主</td><td>时间，技能，消费频次</td></tr></tbody></table><p>我们的目标是提高赚钱能力，一个思路是从单一途径到覆盖三种途径，另外一个思路是想办法突破每个方式的瓶颈。</p><h3 id="一、优化生产"><a href="#一、优化生产" class="headerlink" title="一、优化生产"></a>一、优化生产</h3><p>什么是生产？有完整、有价值、可购买的产出。</p><p>我们大多数人有可能从来没有从事过正经的「生产」工作，因为我们可能没有想过，或者无法独立提供完整的产出。是的，从来！……细思极恐啊！</p><p>但其实绝大多数人都有个人生产的能力。</p><p>比如，门槛较低的内容生产——以文字、语言、视频等方式产出内容。内容生产的「原料」是什么？是经验，是思考，是感受，是对他人有帮助的对话。生产设备/技术/流程包括什么？构思，收集，整理，编写，修改，设计，发表，互动……瓶颈是什么？没有擅长/有积累的可持续创作的话题，拖延，没有读者……</p><p>这些问题很难解决吗？似乎不是，都有很多办法，而且身边容易找到有经验的人。最大的瓶颈大概是一没有开始，二没有持续，三没有改进。</p><p>尽早尝试适合自己的个人生产方式吧，早积累，早收获。</p><h4 id="升级1：产品化"><a href="#升级1：产品化" class="headerlink" title="升级1：产品化"></a>升级1：产品化</h4><p>产出可以分为两种：需要值守的，无需值守的。</p><p>如果每次生产都需要自己在一旁守着，时间还是被占用了啊。所以生产的第一个升级目标是：产品化。<strong>产品化意味着有明确的人群和需求定义，让生产流程和产出都遵循一定标准，以保证产出的稳定，成为可出售的「产品」</strong>。比如把零散的知识和文章整理成一门课程，录制一次，后期就不需要投入太多精力维护。</p><p>想办法让生产过程自动化，构建属于自己的生产系统，这大概就是个人商业模式的核心。</p><h4 id="升级2：关注生产效率"><a href="#升级2：关注生产效率" class="headerlink" title="升级2：关注生产效率"></a>升级2：关注生产效率</h4><p>生产的第一大指标（或者说瓶颈）就是生产效率。认真思考影响生产效率都有哪些因素。有没有可能通过购买的方式提高生产率？另外要注意，所谓效率，一定与时间周期有关，是否给自己设定了合理、可产生回报的生产周期?</p><h4 id="升级3：选择人群和经营渠道"><a href="#升级3：选择人群和经营渠道" class="headerlink" title="升级3：选择人群和经营渠道"></a>升级3：选择人群和经营渠道</h4><p>谁会购买你的产品？他们在哪里聚集？你在那里是否有影响力？如何获取信任？如何与他们互动？</p><h4 id="升级4：企业化"><a href="#升级4：企业化" class="headerlink" title="升级4：企业化"></a>升级4：企业化</h4><p>一个人再怎么提高效率也是有限的。下一步升级就要靠更多人参与了。以公司经营的方式维持产品的生产、运营和增长，将个人商业模式往公司商业模式迁移。所谓企业家，就是找到资源的更优组织方式，为大家的利益解决问题的人。</p><h3 id="二、优化交易"><a href="#二、优化交易" class="headerlink" title="二、优化交易"></a>二、优化交易</h3><p>成功的交易，大概是提前锚定价格差，寻找价值洼地并持有，并在合适时间出售。</p><p>从今天开始，经常问自己一个问题：</p><blockquote><p>什么会在未来很值钱？</p></blockquote><p><img src="https://cdn.dribbble.com/users/828451/screenshots/3088929/hf001_vestly-05.jpg" alt=""></p><p>投资那些未来很可能增值的资产，可能是公司股票，可能是房产，可能是古董，可能是火星矿产，也可能是——人，尤其是，自己。</p><p>好的交易，关乎资本大小，但更重要的是决策信息，而最最关键的是，时机。</p><p>好的决策来自思考质量和经验。建立起自己的思考模型，积累特定领域的知识和投资经验，这些都需要时间投入。选择合适的交易时机，更是需要了解人性的弱点和各种认知偏差，因为投资本来是件由概率这只大手所操控的事件，而概率实在太反直觉了。</p><p>想要获得更高收益，原理其实并不复杂：</p><p>$收益 = 本金 * (1+复合年化收益率)^{年数}$</p><p><strong>所以，要学好概率，练好头脑，控制风险，培养耐心。</strong></p><p>好吧，如何开始，交易的本金从哪里来？没有太多选择的话，只能从生产来。而且最好保证有持续的生产收益，作为交易的本金，而不是只依靠交易作为唯一的赚钱途径——因为那样太容易影响交易的心态和判断。</p><h3 id="三、优化服务"><a href="#三、优化服务" class="headerlink" title="三、优化服务"></a>三、优化服务</h3><p>服务其实是最特殊的挣钱方式，必需通过本人和服务过程去完成。因为在三种途径里面，这是最消耗时间的，所以更加应该找到优化的方式。</p><p>提高服务的收益的一个可能是，组织一个服务平台，聚集服务供需双方，把服务转化成生产。不过这又属于生产的话题了。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/illustration-04.jpg" alt=""></p><p>既然时间无法复制，那就只能打提高单位时间的服务收益的主意了。影响收益的，似乎最终可以归结为一个因素：稀缺性。</p><p>而稀缺性又由两方面影响：</p><ul><li>需求有多迫切，是不是高频、刚需。这就需要研究雇主/买主的需求了。比如招聘的公司处在什么阶段？最需要什么样的人才？</li><li>独特的核心竞争力组合。找单一领域的专家总是相对容易的，找到复合型大咖难度就高很多，同时横跨多个高速发展的领域，只能是炙手可热了。</li></ul><p>所以，尽早投资在需求会持续增长的服务领域，并且培养自己的独特竞争力、跨界能力，其他就交给时间吧！</p><h2 id="财富积累的放大器"><a href="#财富积累的放大器" class="headerlink" title="财富积累的放大器"></a>财富积累的放大器</h2><p>善用一些已经被反复验证过的原则，能够帮助我们放大为获取财富所付出的努力。</p><h3 id="铁律：生产率-gt-收入-gt-负债"><a href="#铁律：生产率-gt-收入-gt-负债" class="headerlink" title="铁律：生产率 &gt; 收入 &gt; 负债"></a>铁律：生产率 &gt; 收入 &gt; 负债</h3><p><strong>尽可能让个人生产率提升的速度，超过收入增长，远远超过负债的增加。</strong></p><p>这几乎就是个人积累财富的全部秘密了。这个不等式出自下面这个视频：</p><p><a href="https://v.qq.com/x/page/z01685nf12f.html" target="_blank" rel="noopener">三十分钟说清经济机器是怎样运行的</a></p><p>出自 Ray Dalio 大佬的视频把经济的底层规律总结得深入浅出。也许很多大学四年的经济学教育，还没有这十分钟的视频内容深刻且有效。墙裂推荐，值得每半年复习一次。</p><h3 id="善用过去的积累"><a href="#善用过去的积累" class="headerlink" title="善用过去的积累"></a>善用过去的积累</h3><p>不要觉得自己是从零开始。</p><p>如果你在职场有几年经验，不要忽视得到过的职业锻炼：自律，沟通和表达能力，学习能力，合作能力，自我/上下/平级的管理能力，等等。把它们通通用在生产和交易上，并且持续打磨这些技能。能服务好别人的人，更懂得如何生产出好的产品。一直练习交易的人，更容易发现应该在哪些领域持续打造自己的核心竞争力。</p><p>去发现那些在某些领域遍地都是、但是在另一个领域稀缺的东西，从中套利。</p><h3 id="尊重时间的复利"><a href="#尊重时间的复利" class="headerlink" title="尊重时间的复利"></a>尊重时间的复利</h3><p>经常回顾指数增长的曲线吧，那是我们的目标曲线——无论是财富增长也好，个人成长也好。</p><p><img src="https://dare2.dk/demo/wp-content/uploads/2015/09/Exponential-curve.png" alt=""><br>via <a href="https://dare2.dk/top-3-steps-for-becoming-an-exponential-organization/" target="_blank" rel="noopener">Top 3 Steps for Becoming an Exponential Organization | DARE2</a></p><p>不要忽视、低估积累期、平台期的长度和难度，但要想办法加快增长速度，尽快到达「奇点」。</p><h3 id="用财富换幸福"><a href="#用财富换幸福" class="headerlink" title="用财富换幸福"></a>用财富换幸福</h3><p>在积累财富的跑道上，不要忘记我们的目标和初心。赚钱可能没有你想象那么难，它可能没有你想象那么重要。（前提是你已经得到了它）</p><p><strong>实现财务自由是为了什么？获得自由的时间要用来做什么？</strong>这才是真正重要的问题。</p><p>努力赚钱之余，<strong>千万不要忘了发现和保护内在动机，去坚持做那些（现在）不但不能帮你赚钱，还需要你补贴钱或时间去做的事情</strong>，因为那些才是你的心头所好啊！我们这么努力，还不是为了能跟那些事情长相厮守在一起？</p><p>最终的最终，我们换取的可能都是用于创造心流的环境。如果现在就有这样的条件，保留一部分时间去体验创作的心流，还有什么理由不开始呢？</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>总结下来，道理其实都十分简单明了，不过 7 个要点：</p><ul><li>财务自由指不需要被动出售自己的时间，这是赚钱的阶段性目标</li><li>思考财富的单位应该是有价值的结果，而不是时间周期</li><li>生产、交易、服务是挣钱的三大途径</li><li>生产优化：让生产过程自动化，构建属于自己的生产系统</li><li>交易优化：形成有预见性的预判，寻找价值洼地并持有，并在合适时间出售</li><li>服务优化：满足高频、刚需的需求，不断积累出独特的核心竞争力组合</li><li>利用四大财富放大器：财富不等式，过去的积累，时间的复利，财富换幸福</li></ul><p>以及需要经常复习：</p><ul><li>一个问题：什么会在未来很值钱？</li><li>一个图表：指数增长</li><li>一个视频：经济是如何运行的</li></ul><p>好了，我想到的是这些了。</p><p>最后，对自己说：想明白了就去践行吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/dzw03-title.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="CriticalThinking" scheme="http://uegeek.com/tags/CriticalThinking/"/>
    
      <category term="大哉问" scheme="http://uegeek.com/tags/%E5%A4%A7%E5%93%89%E9%97%AE/"/>
    
      <category term="Love" scheme="http://uegeek.com/tags/Love/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：Linear regression 线性回归</title>
    <link href="http://uegeek.com/171213DLN3-LinearRegression.html"/>
    <id>http://uegeek.com/171213DLN3-LinearRegression.html</id>
    <published>2017-12-13T11:02:19.000Z</published>
    <updated>2017-12-18T11:20:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p><code>阿扣</code>：今天带你了解一下线性回归。</p><p><code>阿特</code>：🙄 听起来就不是什么容易懂的东西……为什么要了解线……什么，线性回归呢？</p><p><code>阿扣</code>：什么<a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">机器学习啊深度学习啊</a>，最终目的之一不就是<strong>根据已有数据做出预测</strong>，回归和分类都是「做预测」的主要手段。在下面这张图中找找看，线性回归在机器学习中的位置：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/LinearRegressionInML.png" alt=""></p><p><code>阿特</code>：如果说目的都是做「预测」，回归分析和分类有什么不同呢?</p><p><code>阿扣</code>：<strong>回归得到预测的具体数值</strong>，比如股市的行情、未来的气温值。<strong>而分类得到一个「声明」，或者说对数据打上的标签</strong>。</p><p><code>阿特</code>：那什么是线性回归呢？</p><p><code>阿扣</code>：线性回归是最基础的回归类型，它的定义是这样：</p><blockquote><p>在统计学中，线性回归（Linear regression）是利用线性回归方程的最小平方函数，对一个或多个自变量和因变量之间关系建模的一种回归分析。这种函数是一个或多个回归系数的模型参数的线性组合。</p></blockquote><p><code>阿特</code>：好吧，看不懂……不过我主要不明白的是「回归」的意思，要回哪里哦……</p><p><code>阿扣</code>：初中时学的解方程还记得吧？方程左边有 X，求方程右边的 Y： ax + b =y 。</p><p><code>阿特</code>：这个还是记得的。</p><p><code>阿扣</code>：回归分析假设 X 和 Y 之间是有奸情哦不对是有关系的，用于了解只有一个自变量 X 时，因变量 Y 的变化。</p><ul><li>鬼话版：回归分析用来估计模型的参数，以便最好地拟合数据</li><li>人话版：「回归」的目的呢，就是<strong>找出一个最能够代表所有观测数据的函数，来表示 X 和 Y 的关系</strong>。这个函数只有一个变量，所以是类似这样的一条直线：</li></ul><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/640px-Linear_regression.svg.png?1512632790654" alt=""></p><p><code>阿特</code>：好像我记得那种方程在坐标轴上就是用一条直线来表示。不过怎么基于这条直线做预测呢？</p><p><code>阿扣</code>：其实不是基于这条线，而是 <strong>「找出」这条最符合 X 和 Y 的关系的线 (line of best fit)，认定这就是它们之间的「关系」，然后去做预测</strong>。</p><p>我们先来用符号把这个 X 和 Y 的关系表达式写出来。A 表示我们手上有的数据集，比如你每天的能量摄入和体重值，哈哈哈，然后可以用它来预测你什么时候会变成个胖纸~</p><p><code>阿特</code>：紧脏……</p><p><code>阿扣</code>：来看看这张图，我告诉你每个字母代表什么：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/linearClassifier1.png" alt=""></p><p><code>X</code> 是每天的能量摄入，<code>y</code> 是体重。我们想预测你的未来体重 $\hat y$ (给字母加个帽子一般表示它的预测值)，于是用 能量输入 乘以一个权重(weight) <code>W</code>，加上一个偏置项(bias) <code>b</code>，就是计算体重的函数了。</p><p>$$WX + b = y$$</p><p><code>阿特</code>：好像蛮简单的。</p><p><code>阿扣</code>：是啊。这个式子以后我们还会无数次看到，是老朋友来的。</p><p>关于回归分析，再多说两句。</p><p><code>阿特</code>：我有预感不止 20 句……</p><p><code>阿扣</code>：它有三个主要用途：</p><ul><li>因果分析：确定<strong>自变量对因变量的影响的强度</strong>。比如计算剂量和效应，销售和营销支出，年龄和收入之间的关系。</li><li>预测影响：预测影响或变化的影响，即<strong>因变量随着一个或多个自变量的变化而变化多少</strong>。典型的问题是，「增加一个单位 X， Y 能增加多少？」</li><li>趋势预测：<strong>预测趋势和未来价值</strong>。比如，「从现在起6个月，黄金的价格是多少？」，「任务 X 的总体成本是多少？」</li></ul><p><code>阿特</code>：好像很强大，那它有什么缺点呢？</p><p><code>阿扣</code>：有两个主要的缺点：</p><ul><li>只适用于本身是线性关系的数据</li><li>对 outliner 敏感</li></ul><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/lin-reg-w-outliers.png" alt=""></p><p>比如上图右上角的几个点，偏离平局值比较多，我们叫 outliner。出现这种情况，我们可以试试其他的回归分析类型，或者放弃回归分析，用其他的算法了。</p><table><thead><tr><th>Name</th><th>名称</th><th>因变量个数</th><th>自变量个数</th></tr></thead><tbody><tr><td>Simple linear regression</td><td>简单线性回归</td><td>1</td><td>1</td></tr><tr><td>Multiple linear regression</td><td>多元线性回归</td><td>1</td><td>2+</td></tr><tr><td>Logistic regression</td><td>逻辑回归</td><td>1</td><td>2+</td></tr><tr><td>Ordinal regression</td><td>序数回归</td><td>1</td><td>1+</td></tr><tr><td>Multinominal regression</td><td>多项式回归</td><td>1</td><td>1+</td></tr><tr><td>Discriminant analysis</td><td>判别分析</td><td>1</td><td>1+</td></tr></tbody></table><p>如果需要预测的结果依赖于多个变量，可以用多元线性回归，比如：</p><p>$$y = m_1x_1 + m_2x_2 + b$$</p><p>我们用一个三维平面来表示这个二元线性回归：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/just-a-2d-reg.png" alt=""></p><p><code>阿特</code>：那么多回归类型，不会都要掌握吧？</p><p><code>阿扣</code>：嗯，我们接触比较多的是逻辑回归(Logistic regression)。下回给你讲讲逻辑回归要用到的激活函数吧。</p><p><code>阿特</code>：🐵 </p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://www.wikiwand.com/zh/%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90" target="_blank" rel="noopener">迴歸分析 - Wikiwand</a></li><li><a href="https://www.wikiwand.com/zh/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8" target="_blank" rel="noopener">線性回歸 - Wikiwand</a></li><li><a href="http://www.statisticssolutions.com/what-is-linear-regression/" target="_blank" rel="noopener">What is Linear Regression? - Statistics Solutions</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：Neural Networks 神经网络</title>
    <link href="http://uegeek.com/171209DLN2-NeuralNetworks.html"/>
    <id>http://uegeek.com/171209DLN2-NeuralNetworks.html</id>
    <published>2017-12-09T11:01:34.000Z</published>
    <updated>2017-12-18T11:17:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png" alt=""></p><a id="more"></a> <p>回顾 - <a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">DL笔记：机器学习和深度学习的区别</a></p><p><code>阿特</code>：听说深度学习的思想受到神经网络的启发，那是什么玩意儿？</p><p><code>阿扣</code>：神经网络包括生物神经网络和人工神经网络。在生物神经网络中，每个神经元与其他神经元相连。它接收其他神经元的输入，当电位超过了某个阈值（threshold）而被「激活」时，会向相连的神经元「发射」（fire）信号。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/neuralNetwork.png" alt=""></p><p><code>阿特</code>：那跟<a href="http://www.uegeek.com/171206DLNote1-ML-DL-Basic.html" target="_blank" rel="noopener">机器学习</a>有关系吗？机器没有生命啊……</p><h3 id="Perceptrons-感知机"><a href="#Perceptrons-感知机" class="headerlink" title="Perceptrons 感知机"></a>Perceptrons 感知机</h3><p><code>阿扣</code>：参考生物神经网络，在计算机科学中，我们将独立的计算单元看做神经元。感知机 (Perceptron) 是神经网络的基本单位。每一个感知机都完成类似「给我一个数字，我告诉你它是正还是负」这样的简单任务。</p><p>比如说，我们把神经元看做包含一个 0 到 1 之间数字的小球：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/perceptron-s.png" alt=""></p><p>神经元里面的数字叫激活函数 (Activation)。当数字超过某个阈值，比如说 0.5 时，我们就说这个神经元被激活了，它会输出 1 作为信号。如果神经元包含的数字小于 0.5，那它就输出 0，表示没有被激活。</p><p>这个神经元就是一个感知机。</p><p>一个感知机接收若干二进制输入 $x_1,x_2,…$，然后产生一个二进制输出：</p><p><img src="http://neuralnetworksanddeeplearning.com/images/tikz0.png" alt=""></p><p><code>阿特</code>：这小球长得倒是有那么一丢丢像神经元……</p><p><code>阿扣</code>：在这个最简单的系统里，包含：</p><ul><li>输入：这个神经元接收到的其他神经元的信号</li><li>判断器：激活函数</li><li>输出：1 表示 yes「发射」，0 表示 no「不发射」</li></ul><p><code>阿特</code>：艾玛，这也叫简单？</p><p><code>阿扣</code>：它其实是这个意思：</p><p><img src="https://viniciusarruda.github.io/images/mp_neuron.png" alt=""></p><p><code>阿特</code>：好吧我错了……让我晕一晕</p><p><code>阿扣</code>：其实主要看蓝色的字就好。神经元怎么计算输出呢？我们引入「权重」(weights)，它表示从输入到输出的重要程度。权重的和 $\sum_j w_jx_j$ 如果大于阈值 $v_k$，就输出 1。</p><p>每一层神经元因为拥有上一层神经元的「经验」（上一层的输出），所以可以做出更抽象的「决策」。当我们把许多这样的神经元按一定的层次结构连接起来，就得到了人工神经网络（Artificial Neural Network）。</p><p><code>阿特</code>：ANN，那我可以叫它 安？</p><p><code>阿扣</code>：你喜欢咯…… 其实所有的深度学习的神经网络，都可以抽象成三个部分：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/neuralNetwork1.png" alt=""></p><p>除了输入和输出层，中间的层都叫隐层。<strong>深度神经网络就是隐层数量很多的神经网络，深度学习就是从多层神经网络中，自动学习出各种 pattern。</strong></p><p><code>阿特</code>：666！能不能 input 废纸 output 比特币呀？</p><p><code>阿扣</code>：……吃药时间到了</p><h3 id="利用深度神经网络进行学习"><a href="#利用深度神经网络进行学习" class="headerlink" title="利用深度神经网络进行学习"></a>利用深度神经网络进行学习</h3><p><code>阿扣</code>：总结一下，对神经网络来说，<strong>输入层是数据集/变量，隐层是变量之间的关系（包含变量权重），形成高一级别的「模式」传递给下一个隐层，最后确定输出层的结果。</strong></p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/ANN_example.png" alt=""></p><p><code>阿特</code>：为什么我总是听说「训练」神经网络好让它「学习」呢？</p><p><code>阿扣</code>：训练神经网络的目标，其实就是<strong>计算和调整权重 weights，使得模型输出结果最接近真实的数据集。</strong></p><p><code>阿特</code>：好抽象哦……</p><p><code>阿扣</code>：举个例子，我们要预测房价的走势。假设知道房子大小可以预测房价，这个关系就可以用一个神经网络节点（node）来简单估计。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/fangjia_example.png" alt=""></p><p>如果我们知道很多房子的信息怎么办呢？这时候就需要很多的节点，这些节点构成神经网络。房子的多种信息作为输入，房价的预测值作为输出，中间层（可以有多个）是用来计算出前面一层信息的权重，得出一定的模式，传导给下一层，直到最后得出预测值 y。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/ngcourse_housingprice.png" alt=""></p><p>via: <a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">Neural Networks and Deep Learning | Coursera</a></p><p><code>阿特</code>：好像有点明白了，让机器自己学习中间隐藏起来看不见的「规律」！</p><p><code>阿扣</code>：再举个例子，图像识别是深度学习最广泛的应用之一，我们给系统看一张图，它能告诉我们这张图里有没有汪星人：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/HowToRecognizeDog.png" alt=""></p><p><code>阿特</code>：哇，原来机器在背后做了这么多事情，我还以为机器都很聪明呢，原来它们只是比较勤奋哈哈哈</p><p><code>阿扣</code>：你得到了它~</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">Neural networks and deep learning</a></li><li><a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101" target="_blank" rel="noopener">Deep Learning Nanodegree | Udacity</a></li><li><a href="https://www.coursera.org/learn/neural-networks-deep-learning" target="_blank" rel="noopener">Neural Networks and Deep Learning | Coursera</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/Art_Code_Bro.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>DL笔记：机器学习和深度学习的区别</title>
    <link href="http://uegeek.com/171206DLNote1-ML-DL-Basic.html"/>
    <id>http://uegeek.com/171206DLNote1-ML-DL-Basic.html</id>
    <published>2017-12-06T01:24:46.000Z</published>
    <updated>2017-12-09T01:27:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png" alt=""><br>via <a href="https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/" target="_blank" rel="noopener">The Difference Between AI, Machine Learning, and Deep Learning? | NVIDIA Blog</a></p><a id="more"></a> <p>Nvidia 博客上的这张图很好表示了 AI, Machine Learning, Deep Learning 三者的关系。人工智能是一类非常广泛的问题，机器学习是其中一个重要领域和手段，<strong>深度学习则是机器学习的一个分支</strong>。在很多人工智能问题上，深度学习的方法突破了传统机器学习的瓶颈，因而影响力迅速扩大。</p><h3 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h3><p><img src="https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png" alt=""></p><p>00 试着翻出一些机器学习相对权威的定义，看看它们有什么共同点：</p><table><thead><tr><th>Definition</th><th>Translation</th><th>Source</th><th>Key words</th></tr></thead><tbody><tr><td>The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience.</td><td>机器学习聚焦于一个问题：如何构建随着经验而自动改进的计算机程序。</td><td>Tom Mitchell in  <a href="http://www.amazon.com/dp/0070428077?tag=inspiredalgor-20" target="_blank" rel="noopener">Machine Learning</a></td><td>会自我改进的程序</td></tr><tr><td>Vast amounts of data are being generated in many fields, and the statisticians’s job is to make sense of it all: to extract important patterns and trends, and to understand “what the data says”. We call this learning from data.</td><td>从数据中提取重要的模式和规律/趋势</td><td><a href="http://www.amazon.com/dp/0387848576?tag=inspiredalgor-20" target="_blank" rel="noopener">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a></td><td>模式提取</td></tr><tr><td>Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field…</td><td>模式识别和机器学习是一体两面</td><td>Bishop in <a href="http://www.amazon.com/dp/0387310738?tag=inspiredalgor-20" target="_blank" rel="noopener">Pattern Recognition and Machine Learning</a></td><td>模式识别</td></tr><tr><td>Machine Learning is the training of a model from data that generalizes a decision against a performance measure.</td><td>机器学习是通过用于决策的数据去训练模型，并达到某些运行标准</td><td><a href="http://machinelearningmastery.com/author/jasonb/" target="_blank" rel="noopener">Jason Brownlee</a> in <a href="http://machinelearningmastery.com/what-is-machine-learning/" target="_blank" rel="noopener">What is Machine Learning: A Tour of Authoritative Definitions and a Handy One-Liner You Can Use</a></td><td>通过数据训练模型</td></tr></tbody></table><p>简单来说，就是机器通过一系列「任务」从「经验」（数据）中学习，并且评估「效果」如何：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Col.DL.ETP.png" alt=""></p><p>为什么叫做「学习」呢？一般编程语言的做法，是定义每一步指令，逐一执行并最终达到目标。而机器学习则相反，先定义好输出，然后程序自动「学习」出达到目标的「步骤」。</p><p>机器学习可以分为：</p><ul><li>监督学习：给出定义好的标签，程序「学习」标签和数据之间的映射关系</li><li>非监督学习：没有标签的数据集</li><li>强化学习：达到目标会有正向反馈</li></ul><p><img src="https://i1.wp.com/cybrml.com/wp-content/uploads/2017/01/MachineLearningDiagram.png?resize=770%2C551" alt=""></p><h3 id="机器学习擅长做什么？"><a href="#机器学习擅长做什么？" class="headerlink" title="机器学习擅长做什么？"></a>机器学习擅长做什么？</h3><p>当然是替代重复的人工劳动，用机器自动从大量数据中识别模式——也就是「套路」啦。知道「套路」后，我们可以干嘛呢？</p><ul><li>Classification 分类，如垃圾邮件识别(detection, ranking)</li><li>Regression 回归，例如股市预测</li><li>Clustering 聚类，如 iPhoto 按人分组</li><li>Rule Extraction 规则提取，如数据挖掘</li></ul><p>比如垃圾邮件识别的问题，做法是先从每一封邮件中抽取出对识别结果可能有影响的因素（称为特征 feature），比如发件地址、邮件标题、收件人数量等等。然后使用算法去训练数据中每个特征和预测结果的相关度，最终得到可以预测结果的特征。</p><p>算法再强大，如果无法从数据中「学习到」更好的特征表达，也是徒劳。同样的数据，使用不同的表达方法，可能会极大影响问题的难度。一旦解决了数据表达和特征提取问题，很多人工智能任务也就迎刃而解。</p><h3 id="为什么需要深度学习？"><a href="#为什么需要深度学习？" class="headerlink" title="为什么需要深度学习？"></a>为什么需要深度学习？</h3><p>但是对机器学习来说，特征提取并不简单。特征工程往往需要人工投入大量时间去研究和调整，就好像原本应该机器解决的问题，却需要人一直在旁边搀扶。</p><p>深度学习便是解决特征提取问题的一个机器学习分支。它可以自动学习特征和任务之间的关联，还能从简单特征中提取复杂的特征。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/Col.DL.ML_vs_DL.png" alt=""></p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="http://machinelearningmastery.com/what-is-machine-learning/" target="_blank" rel="noopener">What is Machine Learning: A Tour of Authoritative Definitions and a Handy One-Liner You Can Use - Machine Learning Mastery</a></li><li><a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">机器学习 (豆瓣)</a></li><li><a href="https://book.douban.com/subject/26976457/" target="_blank" rel="noopener">Tensorflow：实战Google深度学习框架 (豆瓣)</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;via &lt;a href=&quot;https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Difference Between AI, Machine Learning, and Deep Learning? | NVIDIA Blog&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="ArtxCode" scheme="http://uegeek.com/tags/ArtxCode/"/>
    
      <category term="DeepLearning" scheme="http://uegeek.com/tags/DeepLearning/"/>
    
      <category term="Coding" scheme="http://uegeek.com/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>大哉问02 - 如何爱自己？拟一份爱的宣言</title>
    <link href="http://uegeek.com/171204HowToLoveYourself.html"/>
    <id>http://uegeek.com/171204HowToLoveYourself.html</id>
    <published>2017-12-04T01:16:18.000Z</published>
    <updated>2017-12-09T01:23:32.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/love_yourself.jpg" alt=""></p><a id="more"></a>  <p>这是一个困扰我已久的问题。</p><p>以前，朋友会跟我说「你不够爱自己」。除了愕然，好像多少有些认同，可是再进一步也不知道该做些什么。最近这大半年近距离接触了心理咨询，每次看到「爱己才能爱人」这个几乎是所有情感问题解药中的配方出现，也就时不时会想起这个问题。</p><p>可是，「爱自己」这简单的三个字，到底要怎么做？</p><h2 id="烦恼之源"><a href="#烦恼之源" class="headerlink" title="烦恼之源"></a>烦恼之源</h2><p>阿德勒说，所有的烦恼都来自于人际关系。我想，他说的人际关系，既包含与他人的关系，但首先是与自己的关系。在所有人际关系问题的表象之下，根源也许都是我们与自己的关系出现了紧张。</p><p>我们容易焦虑。且不说在焦虑的时候，「自己」在不在场，有没有提供应有的安慰。更棘手的是，这些焦虑的来源，往往就是我们自己。</p><p>所以我们感到痛苦，想要逃避，不断寻求一种来自外部的肯定，确认自己是有价值的、好的、受欢迎的、值得被爱的。在困顿无助的时候，因为自己给不出，所以时时想伸手向外寻求安抚。</p><p>我们甚至不知道自己要什么，不与自己耐心对话，而是下意识地、急不可耐地抓过一些能彰显某种身份的标识装扮在身上，尝试告诉自己和世界，我是一个怎样的人，你们应该怎样对待我。</p><p>阿德勒给出的解药是：人只有在觉得自己有价值的时候，才会有勇气。而价值不由「做了什么」来提供，那是有条件的价值。如果我们能以「存在」的视角来看待自己，认可存在本身就是价值，就能使自我的关系更加和谐。</p><p>跟别人的关系，再亲密也好，也有可能或主动或被动终结。但是自己跟自己相处一辈子，无法欺瞒，无法离弃。处理好跟自己的关系，真真是人生必修第一课。</p><p>如果用一种比喻形容你和自己的关系，会是什么？</p><p>对「如何爱自己」这个问题，这会是一个好的开始。</p><h2 id="从爱他人学着爱自己"><a href="#从爱他人学着爱自己" class="headerlink" title="从爱他人学着爱自己"></a>从爱他人学着爱自己</h2><p>「爱自己，就给自己买 xxx」</p><p>这大概是时下最流行的广告语。如果爱自己只是愿意花钱，只是时时放过自己，那未免也太容易了，我怎么一直没学会呢？</p><p>可能因为，<strong>爱是一种需要付出艰辛努力和持续练习才能获得的能力。</strong></p><p>如果一个问题我没法理解，就会用「类比和迁移」的办法。比如，先去想想我是怎么喜爱某个兴趣的，怎么把这种喜爱转换到自己身上。</p><p>但是行不通。因为「爱」是一个关系和相处问题，不能缺少人这个因素，无法只靠逻辑去求解。</p><p>似乎只能选择与你关系最亲密的人作为对象，把经验迁移过来：</p><blockquote><p>你如何去爱一个人？</p></blockquote><p>当你有要爱的人，就会变得勇敢和愿意付出。想想你会如何把最好的事物和感情交付给所爱的人？尤其当这个人需要你保护和付出时，比如你已经为人父母，选择小孩为对象来类比和迁移经验，就再适合不过了。</p><p>有时候问题就是这么吊诡——当我们不会爱别人，我们可能就不会爱自己；当我们不爱自己，就没法很好地爱别人。到底从哪一端开始？就从现在所处的位置开始吧，以一个新的角度看待与自己的关系。</p><h2 id="爱是一种练习"><a href="#爱是一种练习" class="headerlink" title="爱是一种练习"></a>爱是一种练习</h2><p>什么是爱？<strong>弗洛姆说：爱是一种能自觉地为被爱者的发展和幸福而付出一切努力的代称。</strong></p><p>让我们暂时把这句话作为「爱自己」的总纲领吧。</p><p>可是只有纲领还不够，我们还要细细去追问，我们可以如何爱人，我们应该如何爱自己。以下的铺陈，不是已经掌握的能力，而是「爱他人和爱自己」的详细理解，是行动纲领，是愿景，是坚持不懈的练习目标。</p><h3 id="忠于自己"><a href="#忠于自己" class="headerlink" title="忠于自己"></a>忠于自己</h3><p>世间有千万种勇气，最不能丧失的一种，叫做「直面自己」。这是对爱情也不能让步的。</p><p>每个人都需要面对自己过去生命里的困顿、失落、挫折。有时候这些负担太沉重，不希望它们如影随形，所以我们选择打造一个面具，或者任由「超我」去规划和引导一条偏离原本轨迹的人生航线。越是掩埋过去，离真实的自己好像就越遥远。失去真实，力量就会耗费在与自己的斗争中，无暇照顾其他。</p><p><strong>接受对方的一切，只要那是真实的。——这大概是爱他人和爱自己的第一原则</strong></p><p>台湾作家陈雪有一段话说得好：</p><blockquote><p>爱是即使在孤独中依然可以付出力量，爱是：我珍爱你的脆弱孤独，你的别扭，你的生硬，你的艰难，爱是正因为我知道我可以穿透那些硬壳看见最脆弱的你，那无意间暴露在我眼前的，使我想要细心爱怜。</p></blockquote><p>使人相爱的最终是安全和信任，而不应该是幻想成分居多的期望。</p><p>因为爱你，所以信任你，希望你在我身边可以做自己，像草木一样自如。因为爱你，所以想要陪你去细细区分，哪些是内心真正认同的；哪些是过去的伤害所结的伤疤，不想去触碰；哪些习性反应是即使伤口早已愈合，却还是习得性回避的东西。</p><p>因为爱自己，所以给自己安全和信任，而不是持续地声明种种期望。时时留心公平地对待本我/自我/超我，不过分关注超我，因为那样反而滋长它。接受本我，不去对抗一些出厂设置。让自我积极探索，回答「我是谁」的问题。</p><p>来自底层、最为无条件的信任，只能自己供给。<strong>接受任何面貌的自己，不推诿，不辩解，不自我攻击。</strong>在主动成长之前，不要求，不期待，然后才会放胆去探索。<strong>放弃要求并不意味着放任自流，因为你相信自己会选择合适的道路，会对自己负责，会匹配得起生命本身应有的重量。</strong></p><h3 id="尊重"><a href="#尊重" class="headerlink" title="尊重"></a>尊重</h3><p>因为爱你，所以视你为一个独立而完整的人。相信你有能力解决属于你的问题，探索出一条未知但属于你的道路。我作为一个旅伴，不去动那些想改变你的念头，即便觉得那些改变可能对你比较好——因为只有你能决定哪些是真正的好，哪些改变你愿意发生。你是自己的主导者，别人不能代替你成为自己，谁也不行。如果你愿意，如果你向我表达，我很乐意陪你一起探索。</p><p>因为爱自己，我视自己为一个独立而完整的人。别人不能代替我成为自己，谁也不行。</p><p><strong>对自己负起全部的责任。</strong></p><p>我对自己说。</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p>开始喜欢一个人时，最直观的疯狂和努力大概就是「了解」。</p><p>了解对方的喜好，看什么书和电影，喜欢吃什么，去过的地方，值得骄傲的经历，以后的打算……日常再琐碎的细节，都会被仔仔细细、反反复复地探究。我们期望通过这些探究，建立起关于对方的庞大数据库，以便加快关于这人的一切信息的处理速度，识别出各自模式，生成与对方的相处模式——了解慢慢变成了理解。而人又是变化的，所以理解这个工程并没有停歇的一天。</p><p>因为爱你，想用一辈子的时间去理解你，成为关于「你」的专家。我想，当我真正理解你，这段关系的任何走向、任何形式和任何结果，我应该都能接受。因为我会明白你怎么一步一步走到这一天。理解最终带来的是自由吧。</p><p>但是理解很难。</p><p>我们常常用「想象」、「推测」代替理解。<strong>理解需要通过真实场景下真实发生的言语行为，以及隐藏在表象之下的弦外之音、模糊的线索去深入同理、思考、判断、修正。</strong></p><p>因为爱自己，面对自己时选择慢下来，不逃避，有耐心。</p><p>人最擅长自欺欺人，所以真正的理解总是多少伴随着恐惧、失落、争执、误解、孤独。就是因为得来不易，所以这样的心意才显得可贵。比起一时的眉目传情、心意相通，尝试去理解的努力会让人更接近爱，那是一种练习，一种付出，一种耕耘。</p><p>在成为他人的专家之前，祝我们都成为自己的「专家」。</p><h3 id="初心"><a href="#初心" class="headerlink" title="初心"></a>初心</h3><p>爱上一个人，总是有初心的，不论自己能不能说得清楚（大部分情况都说不清楚）。</p><p>那份初心，大概都会来自某种与众不同。</p><p>喜欢你，是因为你敢于坚持某些东西，敢于跟普世保持距离，敢于在某些大家可能忽视的方面做个异类。喜欢你，是因为你拥有独特的生命力，呈现出独特的生命状态。</p><p>因为爱你，所以珍视这些与众不同。希望你能继续保护好那些你不曾妥协、拼命守护的东西，那些在经历起伏后，会感慨好在自己守住了的美好而脆弱的东西。</p><p>因为爱自己，我也要慢慢找出自己一直在努力守护的东西。我们有那么多东西可以妥协，也不得不妥协，偏偏就是这些如此弱小的东西，我们花费了几乎所有要来保护。感谢自己守护住了一些纯粹和好奇，面对新的处境时不后退，努力学习和适应，才有了今天写这篇文章的自己。</p><p>因为爱自己，我会继续打磨三观，让初心跳动到每一个当下，让自己可以时时为它代言。</p><h3 id="相处"><a href="#相处" class="headerlink" title="相处"></a>相处</h3><p>比爱更难的是理解，比理解更难的是相处。安心、长久的陪伴，前提是双方不会想逃离。如果一味严肃、苛责、不会表达、不愿交流，谁都会被吓跑的。</p><p>因为爱你，我愿意倾听，尽力去感受你的情绪和需要，理解发生了什么，你需要面对的是什么，给予温和但有力量的回应，然后才是一起分析和解决问题。不应该指责你做得不够好，因为已经看到了那么努力的尝试。你已经足够努力了，我也不会做得比你好，不过如果你愿意，我们可以一起再试试看。</p><p>因为爱自己，所以事事应该先安慰和鼓励自己——情绪平复以后，问题我会自己解决好的。我不是一个生性敏感而丰富的人，倾听/沟通/共情都需要大量练习。对别人对自己都是，要保持练习。</p><p>因为爱你，应该尽量在相处中让你时时都感到「放松」。首先放下自己的期盼和要求，相信你不是不重视我，只是需要空间先处理好自己的各种事物，先跟自己好好相处。</p><p>所以，跟自己的相处之道，大概也是尽量让自己「放松」。「放松」不是一种命令，甚至无法成为目标，因为它是一种悖论式的存在。<strong>真正的包容，也包容不包容；不要求放松，可能就能放松下来。一旦放松，有趣也就会发生。</strong></p><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>天长地久还是曾经拥有？</p><p><strong>死亡这个事实一早就劝诫我们，生命的过程远远比结果重要。</strong></p><p>爱可能始于一个激动人心的确认和开始，有可能终结于一个无法释怀的离别。但那些都不是爱的主体。爱的主体是两人的关系，建立在经年累月的相处和理解之上的关系，即使不在身边内心也会留有对方位置的关系。<strong>爱既不是最终的结果，也不是开篇的承诺。爱其实存在于关系建立和变化的全程。</strong>所以它一定不如预想的跌宕起伏，因为生活的大多数时光总是平淡的。但爱就是在一起，或身或心。</p><p>喜欢你，是因为你身上有达成某些结果的能力，而不是因为你口袋里装着这些结果。所以，有自我疗愈和成长的能力，比成熟更重要；有对美的理解和坚持，比颜值更重要；是否能鉴别有趣的问题并愿意费心思索，比聪明重要；看待财富的观念和赚钱能力，比有多少钱重要……</p><p>既然爱是过程，何必计较结果。向死而生的人类，在生命流逝的过程中，暂时无视死亡的结果，转而积极地探索可能性，才创造出了种种奇迹。</p><p>因为爱你，所以我选择 being in the present，尽可能心无旁骛地陪伴，让有你的时光加倍值得回味。</p><p><strong>因为爱自己，所以时时跟自己在一起，明白此刻正在经历什么，全身心地投入，未来不迎，当时不杂，既过不恋。</strong></p><p>因为爱自己，所以明白并不是我不好，也不是世界充满险恶无情，而是还没有修炼到很快找到或切换到一个最适合的角度去看待当下。当熟练之后，更多的时间精力就能从警惕、自我保护、应激中节省出来，去好好欣赏路途的景致，结识同样寂寞但心怀好奇的旅伴，去感受这样的旅途带给我什么样独特的感受。</p><h2 id="当我真正开始爱自己"><a href="#当我真正开始爱自己" class="headerlink" title="当我真正开始爱自己"></a>当我真正开始爱自己</h2><p>感谢今年出现在我生命里我喜爱的人。让我知道自己还有能力去爱，让我有动力成为更好的自己。</p><p>感谢今年出现在我生命里我避之不及的人。让我懂得珍惜自己身上那些弥足珍贵的东西。让我懂得人生有限，应该把最美好的自己，留给合适的人和事。</p><p>爱真的不是放肆，也不是克制，而是深思熟虑、身体力行的努力，可能称不出重量的努力。</p><p><strong>爱不应允幸福，爱是原原本本回到自己身上。问问自己，在这个人/自己身边，能否有勇气清楚看待自己，不屈从，不顺应，不自欺，但也不畏惧改变。</strong></p><p>最后，用卓别林给自己的诗来正式开启爱自己的旅途吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">当我真正开始爱自己，</span><br><span class="line">我才认识到，所有的痛苦和情感的折磨，</span><br><span class="line">都只是提醒我：活着，不要违背自己的本心。</span><br><span class="line">今天我明白了，这叫做「真实」。</span><br><span class="line"></span><br><span class="line">当我真正开始爱自己，</span><br><span class="line">我才懂得，把自己的愿望强加于人，</span><br><span class="line">是多么的无礼，就算我知道，时机并不成熟，</span><br><span class="line">那人也还没有做好准备，</span><br><span class="line">就算那个人就是我自己，</span><br><span class="line">今天我明白了，这叫做「尊重」。</span><br><span class="line"></span><br><span class="line">当我开始爱自己，</span><br><span class="line">我不再渴求不同的人生，</span><br><span class="line">我知道任何发生在我身边的事情，</span><br><span class="line">都是对我成长的邀请。</span><br><span class="line">如今，我称之为「成熟」。</span><br><span class="line"></span><br><span class="line">当我开始真正爱自己，</span><br><span class="line">我才明白，我其实一直都在正确的时间，</span><br><span class="line">正确的地方，发生的一切都恰如其分。</span><br><span class="line">由此我得以平静。</span><br><span class="line">今天我明白了，这叫做「自信」。</span><br><span class="line"></span><br><span class="line">当我开始真正爱自己，</span><br><span class="line">我不再牺牲自己的自由时间，</span><br><span class="line">不再去勾画什么宏伟的明天。</span><br><span class="line">今天我只做有趣和快乐的事，</span><br><span class="line">做自己热爱，让心欢喜的事，</span><br><span class="line">用我的方式，以我的韵律。</span><br><span class="line">今天我明白了，这叫做「单纯」。</span><br><span class="line"></span><br><span class="line">当我开始真正爱自己，</span><br><span class="line">我开始远离一切不健康的东西。</span><br><span class="line">不论是饮食和人物，还是事情和环境，</span><br><span class="line">我远离一切让我远离本真的东西。</span><br><span class="line">从前我把这叫做「追求健康的自私自利」，</span><br><span class="line">但今天我明白了，这是「自爱」。</span><br><span class="line"></span><br><span class="line">当我开始真正爱自己，</span><br><span class="line">我不再总想着要永远正确，不犯错误。</span><br><span class="line">我今天明白了，这叫做「谦逊」。</span><br><span class="line"></span><br><span class="line">我当开始真正爱自己，</span><br><span class="line">我不再继续沉溺于过去，</span><br><span class="line">也不再为明天而忧虑，</span><br><span class="line">现在我只活在一切正在发生的当下，</span><br><span class="line">今天，我活在此时此地，</span><br><span class="line">如此日复一日。这就叫「完美」。</span><br><span class="line"></span><br><span class="line">当我开始真正爱自己，</span><br><span class="line">我明白，我的思虑让我变得贫乏和病态，</span><br><span class="line">但当我唤起了心灵的力量，</span><br><span class="line">理智就变成了一个重要的伙伴，</span><br><span class="line">这种组合我称之为，「心的智慧」。</span><br><span class="line"></span><br><span class="line">我们无须再害怕自己和他人的分歧，</span><br><span class="line">矛盾和问题，</span><br><span class="line">因为即使星星有时也会碰在一起，</span><br><span class="line">形成新的世界，</span><br><span class="line">今天我明白，</span><br><span class="line">这就是「生命」。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/love_yourself.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="CriticalThinking" scheme="http://uegeek.com/tags/CriticalThinking/"/>
    
      <category term="大哉问" scheme="http://uegeek.com/tags/%E5%A4%A7%E5%93%89%E9%97%AE/"/>
    
      <category term="Love" scheme="http://uegeek.com/tags/Love/"/>
    
  </entry>
  
  <entry>
    <title>大哉问01 - 什么样的时间观值得拥有？</title>
    <link href="http://uegeek.com/171126TimePerspective.html"/>
    <id>http://uegeek.com/171126TimePerspective.html</id>
    <published>2017-11-26T01:12:43.000Z</published>
    <updated>2017-12-09T01:14:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/dbproductivity.jpg" alt=""></p><a id="more"></a>  <blockquote><p>我们自己虚构了一些问题,然后又炮制了一些答案。        </p><p>——西蒙娜·德·波伏娃</p></blockquote><p><a href="http://www.uegeek.com/171112-HowToAskGoodQuestion.html" target="_blank" rel="noopener">HackYourself 大哉问系列</a>第一篇，我们来讨论「时间」。</p><p>为什么要选择这么虚无的话题呢？我在想，人大概有三大限制：时间，空间，认知资源。从这三个角度重新思考我们这种脆弱又作死的物种，大概会挺有意思。</p><p>虽然一直身处于奔流的时间长河中，但是我们对时间的汹涌却（常常）浑然不觉。面对时间这种无人可以掌控的东西，不禁想问：</p><blockquote><p>我们应该持有什么样的时间观？</p></blockquote><p>时间观是关于时间的观念，关于时间和自己的关系，如何理解时间，如果依据时间做出种种决策，如何对待时间…… 这里讨论的不是时间管理。毕竟，如果对时间本身都还没有自己的理解，凭什么说人家已经是「朋友」，又怎么去管理它呢？</p><p>心理学家津巴多对「时间」做了三十多年的研究，他呼吁我们认真对待自己所持有的时间观念：</p><blockquote><p>一个健康的时间观，能让你以人生的长度来决定如何做最优化的决策。一个平衡的时间观，是高度的积极的过去观，中等偏高的积极的未来观，以及温和的选择性地享乐但不冲动反应的现在观。换言之，拥有一个平衡的时间观，就是为自己的过去骄傲，有自信；对未来有高期望但并不好高骛远，对于当下的自己，适当地享受生活，但既不冲动也不是随叫随到的无计划性。 ——津巴多</p></blockquote><p><img src="https://cdn.dribbble.com/users/89889/screenshots/3658659/toolittletime-dribbble.gif" alt=""></p><h2 id="给时间重新找一个比喻"><a href="#给时间重新找一个比喻" class="headerlink" title="给时间重新找一个比喻"></a>给时间重新找一个比喻</h2><p>对我而言，时间是什么？</p><p>除了从小被教育的「时间就是金钱」，除了日渐步入「Time famine」的深渊，我对时间最大的印象大概就是「无情」了，像器物一般的无情。</p><h3 id="意义-行动标尺"><a href="#意义-行动标尺" class="headerlink" title="意义/行动标尺"></a>意义/行动标尺</h3><p>与无限的时间相比，人太弱小太可怜，不得不沿着单向、不可逆的时间轴往前奔走。如果不是因为生命的时间有限，谁会在意生存、变化、权力、美、爱、有没有后代？印刻在出厂设置中的生命长度，让（有自我意识的）生命体开始珍视自己，有了尽力让生命存在、焕发的意念，也就滋长了种种欲望。<strong>所有的欲望，都是「生命有限」这个事实的形容词。</strong></p><p>因为生命短促，人才会孜孜不倦地追求目标和意义，好让这短暂的时间之旅的残存能够消散得慢一点。</p><p>看起来好像是时间定义了生命，其实，是生命所的持续时间让人有必要以人的尺度来定义时间：一万年对人来说不重要，一豪秒对人来说也不重要。</p><p>我们太习惯以人活着能够经常体验到的时间单位去观察万事万物，也太习惯以满足当下的需要为理由来消耗时间。这同时也提醒我们，只需要稍微转动一下时间标尺的角度，我们观察世界的框架可能就会大有不同：</p><blockquote><p>如果有一辈子的时间来做某件事会怎样？</p><p>如果某件事只会持续 1 秒，我对它的态度会有什么不同？</p><p>如果我与某个人共有 +∞ 的时间（一直相处）会怎样？如果共有时间为 0 （没有交集）会怎样？</p></blockquote><h3 id="可能性的培养皿"><a href="#可能性的培养皿" class="headerlink" title="可能性的培养皿"></a>可能性的培养皿</h3><p>时间可能是全能的神 最大/唯一武器。</p><p>因为无限，所以造物主根本不需要呕心沥血去「设计」生命的所有细节，而是将一切都交给时间，给定初始值，无限演化下去。不论过程中出现了什么，生命轮回也好，沧海桑田也罢，生机勃勃也好，万籁俱寂也罢，都没有所谓，都只是演化过程的一个片段。</p><p>没有秘诀。</p><p>上帝甚至不需要全知全能——只要交给无限的时间就好。只要时间足够长，可能性就不会穷尽。</p><p>我们常常说：「我没有时间了」、「时间不够」、「你有空吗？」……</p><p>有趣的是，<strong>时间本身不包含任何东西</strong>。时间只是度量单位，只是「容器」。我们问「你有时间吗？」，是不是好像在问：「你有厘米吗？」、「你有分贝吗？」。里面什么都没有。</p><p>真正的内容，是时间单位内我们投入的注意力、能量、情感、行动等等。我们不拥有时间，我们只拥有注意力、能量、情感……时间只是培养皿，想要培养出有机体，需要加培养液，不是吗？</p><h2 id="有意义的时间观"><a href="#有意义的时间观" class="headerlink" title="有意义的时间观"></a>有意义的时间观</h2><blockquote><p>什么样的时间观能带来更大收益？</p></blockquote><p>这是一个倾向性很明显的问题。我们似乎得先考虑：</p><ul><li>什么是「收益」？它一定是「结果」吗？（结果可能只是资源，不是目标本身）</li><li>要在多大时间尺度/周期内考虑？</li></ul><p>时间被如此定义，是因为生命体的有限。那生命又是个什么东西？</p><blockquote><p>生命似乎是物质的有序和有规律的行为，它不是完全基于从有序走向无序的倾向，而是部分基于得到保持的现存秩序。……生命有机体如何避免衰退为惰性「平衡」状态呢？通过新陈代谢。……新陈代谢的本质是使有机体成功消除了它活着时不得不产生的所有熵。 —— 薛定谔，「生命是什么」</p></blockquote><p>生命体的伟大之处，在于（一定时间内）抵御了混沌无序的倾向，制造出（或者说吸收了）「负熵」。</p><p>如果回归到这一层含义，对人有意义的「收益」，是不是也可以理解为「有意义的秩序」？</p><p>暂时抛开时间周期的问题，来想想那些时间感消散的时刻。比如说，专注地处于心流状态的时候，在类似做梦这种意识混沌的时候，我们感受不到时间的流逝。身处这些时刻，是不是反而能更真实、直接地触摸到生命本身？</p><p>回到上面的问题：怎样才算最理想的时间收益？对我而言大概是：</p><blockquote><p>在有意识的所有时刻里，都生机勃勃：投入、沉浸、痛并快乐、见过去所未见，体验一个生命可能抵达的深度。更重要的是——最终塑造出自己，并且留下能延续一段时间、传递某种深度的载体。</p></blockquote><p>那么，可以如何去调控时间以及对时间的感觉？如何放大时间的价值？</p><p>似乎有两种办法：<strong>融于当下，或者穿越时间</strong>。</p><p><strong>融于当下</strong>，是指让时间感消失——沉浸到时间里面，<strong>让更多的时间处于聚焦/有序而不是耗散状态</strong>。也就是说，吃饭时就心无旁骛，与伴侣共处时就交心会意，思考时就清理杂乱的欲望，悲伤时就不要阻止眼泪……努力让真正重要的事情的「过程价值」大于「结果价值」。毕竟，我们全身心经历的是每一分每一秒，应该更多地为全程的福祉考虑，而不只是获得结果的那一刹那的高峰体验。</p><p><strong>穿越时间</strong>，是指留下能够抵御时间侵袭的「晶体」，可以是文字、记录、作品、人生信条等等，它是你拼尽全力，从混沌无序中凝练出来的「恰好」和「最好」，它们是（哪怕只有一丢丢）有意义的秩序，可以穿越（哪怕只有一丢丢）时间长河而不被冲刷得面目全非。</p><h2 id="修炼生命力"><a href="#修炼生命力" class="headerlink" title="修炼生命力"></a>修炼生命力</h2><p>于是，时间似乎是焕发生命力的养料和工具套件。</p><p><strong>让我们把时间当做意义过滤器</strong>。哪些事情哪怕只有 1 秒，我们也会珍惜？哪些信息，出现在时间的源头？哪些内容，能真正穿越时间的洗礼？什么目标，值得用大时间周期去追逐？</p><p><strong>让我们把时间当做培养皿</strong>。在时间这个容器中，我们到底需要倾注些什么？希望锻造出什么？投入产出是否匹配？怎样把其他资源转化为时间资源？如果花的每一分钱，都值时间价值；如果花的每一分钟，都超出经济价值，那大概就是世界上最好的投资。</p><p><strong>让我们把时间当做生命过程本身</strong>。在时间流逝中，尽可能去抵御熵增，提炼有意义的秩序，印证我们的存在和延续——不只是基因和生命体的延续，还是探索精神、思考深度的延续，是对自然和美的敬畏的延续。</p><p>这么看来，其实重要的不是时间能送我们到达哪里，真正应该珍惜的，是时间给了我们修炼的机会——<strong>修炼一种在任何情境下都让自己避免熵增、有所获益的能力</strong>，这种神奇能力，大概就是生命力。</p><p>生命那只随机的大手会把我们降落在哪里，没有人知晓。但不论跌落在哪里，都要生生不息，甚至能留下穿越时间的智慧晶体。</p><p>这是生命的责任。</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://book.douban.com/subject/5246820/" target="_blank" rel="noopener">津巴多时间心理学</a></li><li><a href="https://book.douban.com/subject/26309060/" target="_blank" rel="noopener">生命是什么</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/dbproductivity.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="CriticalThinking" scheme="http://uegeek.com/tags/CriticalThinking/"/>
    
      <category term="大哉问" scheme="http://uegeek.com/tags/%E5%A4%A7%E5%93%89%E9%97%AE/"/>
    
      <category term="时间" scheme="http://uegeek.com/tags/%E6%97%B6%E9%97%B4/"/>
    
  </entry>
  
  <entry>
    <title>用问题对话虚无 —— HackYourself 大哉问系列</title>
    <link href="http://uegeek.com/171112-HowToAskGoodQuestion.html"/>
    <id>http://uegeek.com/171112-HowToAskGoodQuestion.html</id>
    <published>2017-11-12T02:52:24.000Z</published>
    <updated>2017-11-14T02:56:15.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/question-title.jpg" alt=""></p><a id="more"></a>  <blockquote><p>我们自己虚构了一些问题,然后又炮制了一些答案。        </p><p>——西蒙娜·德·波伏娃 「人都是要死的」</p></blockquote><p>经过低产的一年，HackYourself 准备恢复（双）周更的频率。</p><p>欢迎围观 00  的新坑：大哉问系列。</p><p>什么是「大哉问」？</p><blockquote><p>林放问礼之本。子曰：“大哉问！礼，与其奢也，宁俭；丧，与其易也，宁戚。” ——论语 八佾篇第三</p></blockquote><p>大哉问（据说）出自论语，意思是「这是一个非常棒的问题」、「你的问题意义重大啊」。</p><p>这会是一个自问自答的系列。正如「学什么」比「怎么学」更重要，「问什么问题」比「如何回答」更重要。在之前的读书会尝试过「以问题驱动」的读书法，效果不错。加上之前有过「问题作为人生地图」的思考，所以有了大哉问系列的想法。</p><p>人生的种种困惑、迷茫、无力、混沌，只会随着复杂度指数上升的世界而加重。用问题作为线索，与趋于无序、混乱的环境展开对话，也许是一条通幽的小径吧。</p><h2 id="为什么要多问好问题？"><a href="#为什么要多问好问题？" class="headerlink" title="为什么要多问好问题？"></a>为什么要多问好问题？</h2><blockquote><p>为什么与问人类如何作出好决策相比，我们更不愿意问人类如何找到好目标？——马奇</p></blockquote><p>人生好像总是有无穷的问题等待解决。</p><p>问题永远解决不完，这其实不是困扰。真正会造成困扰的，是我们没有意识到：<strong>要解答哪些问题，在某种程度上可以自主选择</strong>。用主动的姿态去探索问题，更是一种选择。</p><p>管理大师马奇的发问提醒我们，好问题多么可贵 —— 跟解决方案（决策）相比，更稀缺的是问对的问题（目标）。</p><h3 id="最大化资源配置"><a href="#最大化资源配置" class="headerlink" title="最大化资源配置"></a>最大化资源配置</h3><p>先用一个功利的视角来解读。</p><p>多问好问题的隐含前提是：<strong>人的寿命太短，精力太宝贵</strong>。</p><p>每一个问题都可能是时间和精力的黑洞，如果我们自己不选择要面对、要解答的问题，马上就会被一大堆问题塞满——<strong>它们可能来自雇佣你、跟你做时间交易的 boss，来自无数生命力顽强的模因（Meme），来自无孔不入想让你做出购买决策的广告，来自不停侵犯个人边界的重要或不重要的他人，来自其实与你完全无关但是能逗乐大脑的垃圾信息</strong>……</p><p>如果把精力和注意力比作可以调动的资源，在滔天的信息洪流之中，我们脑子里工作记忆这一丁点儿资源，实在太过贫乏，必需得像个守财奴一样死死守护它。</p><p>资源配置的目的，无非是更好地转化为产出、达成目标。在这里，「目标」是一个大坑。每个人看似都在为生活奔忙，可是每当夜深人静扪心自问：我每天到底都在干嘛？</p><p>嗯，我到底在干嘛 —— 也是一个问题。</p><p>如果能经常自己给自己设定问题，作为「产出/目的」的重要记录和反馈，并且<strong>有意识地、主动</strong>尝试着寻找答案，「精力」这笔账是不是就没那么糊涂？</p><h3 id="构筑意义，抵抗无序和虚无"><a href="#构筑意义，抵抗无序和虚无" class="headerlink" title="构筑意义，抵抗无序和虚无"></a>构筑意义，抵抗无序和虚无</h3><p>再来切换一个不那么功利的视角。</p><p>人生本无意义。不停追问意义，这大概是人的生理缺陷。</p><p>意义无法按图索骥找到，也不能靠机缘偶遇，而是一点一点提炼和构筑出来的。</p><p>让好问题成为人生线索，通过经常追问各种各样的问题，我们会更清楚自己是什么样的人、看重什么、被什么打动、受什么困扰。这也是打磨三观，寻找意义和自我的过程。</p><blockquote><p>What Do I Stand For?</p></blockquote><p>我为谁（什么）代言？？<strong>这是绝对不能交给他人来回答的问题。</strong></p><p>面对这种终极问题，谁不想回避呢？学校也从来不教该如何解答这些问题。可是它们就是阴魂不散啊，似乎每一次逃避，都往虚无多走了一步。到底有没对错？应该坚持什么？每天的所作所为何以为继？</p><p>万事万物都有一个宿命般的终点：无序。要用有限的生命和稀缺的注意力对抗无序，可能真的没有太多办法，我会试着<strong>用问题编制「有序」的骨架，用沉浸和心流附着成为「有序」的血肉</strong>。</p><p>在问题的牵引之下，希望我们都能迭代出令自己满意的答案，交出人生答卷。当然，给自己设计人生问卷，更为重要。</p><h3 id="留下一些有趣的痕迹"><a href="#留下一些有趣的痕迹" class="headerlink" title="留下一些有趣的痕迹"></a>留下一些有趣的痕迹</h3><p>既然问出了问题，总是会试着去思考、试验、回答。有趣的问题，会激发行动，聚集同好。</p><p>如果一直只是做一个思想和内容消费者，似乎也不太有意思，为什么不留下一些痕迹呢？</p><p>如果能把这些问题的思考、迭代过程记录下来，大概也是对自己一个很好的交待吧？</p><h2 id="什么是好问题？"><a href="#什么是好问题？" class="headerlink" title="什么是好问题？"></a>什么是好问题？</h2><p>这本身就是个需要迭代思考的好问题。</p><p>如果只是为了装 x，很容易问出一些「终极问题」，比如「美是什么？」</p><p>在维特根斯坦看来，这些问题本身不成为问题。因为定义和解释是语言的范畴，只是一种语言的单向逻辑，它解决不了语言之外的问题。意义是终极解释，而「美」不存在语言上的终极解释。那些过于倚重范畴的问题、容易变成文字游戏的「终极」问题，我实在驾驭不来，还是少碰为好。</p><p>怎样识别好问题呢？一个思路是用归纳法，去搜集各种各样的问题，选出好的，然后总结出特征。</p><h3 id="好问题的特征"><a href="#好问题的特征" class="headerlink" title="好问题的特征"></a>好问题的特征</h3><p>00 先凭自己的经验和直觉，尝试总结好问题的特征：</p><ul><li>描述清晰</li><li>不容易回答，值得 go deep，能激发更高、更抽象层级的思考</li><li>指向有潜力的探索方向，牵引出有价值的回答，激发行动和带来改变</li><li>开放式，没有唯一答案，不同背景经历的人可能会有迥异的答案</li><li>或者是时代的大问题，或者是超越时空的普适性问题</li><li>可能不会单独出现，而是一组相关问题</li></ul><h3 id="好问题的栗子"><a href="#好问题的栗子" class="headerlink" title="好问题的栗子"></a>好问题的栗子</h3><p><a href="http://edge.org" target="_blank" rel="noopener">Edge</a> 网站每年都会提出一个 Big question，激发知识界的集体思维碰撞，今年已经是第 19 年。作为好问题的参考再合适不过：</p><table><thead><tr><th>YEAR</th><th>ANNUAL QUESTION</th></tr></thead><tbody><tr><td>2017</td><td>What Scientific Term or Concept Ought To Be More Widely Known?</td></tr><tr><td>2016</td><td>What Do You Consider The Most Interesting Recent [Scientific] News? What Makes It Important? </td></tr><tr><td>2015</td><td>What Do You Think about Machines That Think? </td></tr><tr><td>2014</td><td>What Scientific Idea Is Ready for Retirement? </td></tr><tr><td>2013</td><td>What <em>Should</em> We Be Worried about? </td></tr><tr><td>2012</td><td>What Is Your Favorite Deep, Elegant, or Beautiful Explanation? </td></tr><tr><td>2011</td><td>What Scientific Concept Would Improve Everybody’s Cognitive Toolkit? </td></tr><tr><td>2010</td><td>How Is the Internet Changing the Way You Think? </td></tr><tr><td>2009</td><td>What Will Change Everything? </td></tr><tr><td>2008</td><td>What Have You Changed Your Mind about? Why? </td></tr><tr><td>2007</td><td>What Are You Optimistic About? </td></tr><tr><td>2006</td><td>What Is Your Dangerous Idea? </td></tr><tr><td>2005</td><td>What Do You Believe Is True Even Though You Cannot Prove It? </td></tr><tr><td>2004</td><td>What’s Your Law? </td></tr><tr><td>2003</td><td>What Are the Pressing Scientific Issues for the Nation and the World, and What Is Your Advice on How I Can Begin to Deal With Them? </td></tr><tr><td>2002</td><td>What Is Your Question? … Why? </td></tr><tr><td>2001</td><td>What Questions Have Disappeared? What Now? </td></tr><tr><td>2000</td><td>What Is Today’s Most Important Unreported Story? </td></tr><tr><td>1999</td><td>What Is the Most Important Invention in the Past Two Thousand Years </td></tr><tr><td>1998</td><td>What Questions Are You Asking Yourself?</td></tr></tbody></table><p>在「为未知而教，为未来而学」一书中，作者提出了很多关于教育的好问题。他倡导多提出「有生命力的问题」。</p><p><img src="https://img3.doubanio.com/lpic/s28356404.jpg" alt=""></p><p>哲学家威廉·詹姆斯在「The Will to Believe」一文中，区分了「有生命力的假设」与「无生命力的假设」。有生命力的假设，指一个人在对自己而言具有真实性的问题中所发现的、值得尝试的各种可能性。</p><p>「有生命力的问题」与之类似，<strong>指能够为对话提供焦点和重要意义的一些探究性主题</strong>。教育者可以通过多种方式来引导：</p><ul><li>提供中心线索，包含一系列广泛的探究主题，学习者可以对此进行长时间的探究。</li><li>围绕着大概念而非答案来组织教学。比如：植物不同于动物，它们没有感觉系统，植物的各个部分怎么「知道」应该往哪里生长呢？</li><li><p>提出增殖性问题，让学习者积累一定经验后继续提出相应问题。比如：</p><ul><li>New Middle East 有可能实现吗？（地理）</li><li>人类基因组计划：是福是祸？（生物）</li><li>参与第一次世界大战那一代人为何在 20 年内又发动了第二次世界大战？（历史）</li><li>人为什么要结婚？（社会学和人类学）</li><li>什么是爱？（社会学、生物学、心理学和历史学）</li><li>奥运会是否改善了我们的价值观？（跨学科）</li></ul></li><li><p>找到问题的焦点。从提供一个问题的焦点开始：提供一个主题、题目或者对象，具有真实性和启发性。</p></li></ul><p>解决问题天生伴随着「提出问题」或「发现问题」。这一步教育的缺失，还是得靠自己来弥补。</p><p>关于什么是好问题，你有什么想跟大家分享的吗？欢迎留言。</p><h2 id="如何让好问题成为指引"><a href="#如何让好问题成为指引" class="headerlink" title="如何让好问题成为指引"></a>如何让好问题成为指引</h2><p>普利策奖得主、诺贝尔物理学奖获得者 Isidor Rabi 说，大部分目前在孩子放学回家后都会问一句：「你今天学到什么了吗？」但他的妈妈当年问的是：</p><blockquote><p>你今天有没有提出一个好问题？</p></blockquote><p>提问大概跟学习任何技能都一样，需要大量练习。如果能提出 100 个问题，总能选出最好的 10 个吧？多提问，常常反思是否提出了好的问题，提问这门「手艺」也会精湛起来。</p><p>提出问题仅仅是第一步，我们的目的是让问题真正起到穿针引线的作用。</p><h3 id="筛选问题"><a href="#筛选问题" class="headerlink" title="筛选问题"></a>筛选问题</h3><p>当我们试着列出一些问题，很快就会发现，这些问题的层次差别很大。</p><p>有的思考几分钟就能有大概的思路，有的可能穷极一生也给不出满意的答案。00 暂时以输出为目标，用问题思考周期粗暴地替代问题的复杂度/深度，将问题分成用 周/月/年 时间来思考的不同类型。</p><p>比如「编程思维有什么特点？可以如何改善生活？」可能需要至少一个月来思考和实践。</p><p>那么可以把平时收集的问题列表，标记上「周」或者「月」（如果可以作为年度主题就另外考虑了），如果以月为单位，看看能不能拆分为几个以周为单位的小问题，排出优先级，一周一个。</p><p>大哉问系列打算聚焦在这样的问题：</p><ul><li>思考和实践周期在 一周 ~ 五年 的问题</li><li>对知识体系、立场、思考本身有迭代作用</li><li>个人已有相关困惑和经验积累，待梳理总结</li><li>可以启发近期的行动</li></ul><h3 id="琢磨"><a href="#琢磨" class="headerlink" title="琢磨"></a>琢磨</h3><p>筛选好问题之后才是关键的一步。除了大块的工作时间之外，把问题作为最高优先级的事项：频繁加载问题到脑子中，让注意力尽量聚焦，围绕问题去收集信息、展开思路、建立连接、形成观点，等等。</p><p>怎样做到频繁加载问题呢？现在注意力实在太涣散了。我们可以尝试设定一些 Triggers：</p><table><thead><tr><th>IF</th><th>THEN</th></tr></thead><tbody><tr><td>周日午/晚饭后</td><td>挑选下周的问题，写在卡片上</td></tr><tr><td>出门/通勤</td><td>带上问题卡片，写下思路</td></tr><tr><td>跑步/散步</td><td>热身时加载问题，变跑边整理思路，回来写下笔记</td></tr><tr><td>周六</td><td>整理成文</td></tr></tbody></table><p>（注：跑步和散步对我特别有效，可能不适合大部分人）</p><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>如果没有输出，等于没有思考过。</p><p>思考得再深入，也需要反馈。于是必需将思考做阶段性整理和输出，用来评估自己对问题的理解程度，收集大家的反馈，如果能有观点的碰撞就更好了。</p><p>用于收集反馈和评估的问题：</p><ul><li>这个问题是否激发了比较深度的思考？</li><li>思考、实践过程中我有哪些新收获？</li><li>如何转化为观念和行动上的改变？</li><li>发现了哪些待探索和深入的领域？</li><li>输出过程中遇到哪些问题？</li><li>这到底是不是一个好问题？</li><li>其他人有什么思路？</li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><blockquote><p>Judge a man by his questions rather than by his answers. ― Voltaire 伏尔泰</p></blockquote><p>HackYourself 大哉问系列启动，欢迎提出你的好问题~</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://www.edge.org/annual-questions" target="_blank" rel="noopener">Edge.org</a></li><li><a href="https://book.douban.com/subject/26586892/" target="_blank" rel="noopener">为未知而教,为未来而学</a></li><li><a href="http://blog.lifeway.com/explorethebible/blog/5-characteristics-of-a-good-question/" target="_blank" rel="noopener">5 Characteristics of a Good Question</a></li><li><a href="https://www.zhihu.com/question/21706038" target="_blank" rel="noopener">美是什么？是否存在客观的美？ 以及如何问出一个美的问题？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/22810030" target="_blank" rel="noopener">知乎上的好问题有哪些？</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/question-title.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="CriticalThinking" scheme="http://uegeek.com/tags/CriticalThinking/"/>
    
      <category term="大哉问" scheme="http://uegeek.com/tags/%E5%A4%A7%E5%93%89%E9%97%AE/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning-3：如何在 Docker 里切换 Python 版本</title>
    <link href="http://uegeek.com/171021-DL3-How2UsePy3InDocker.html"/>
    <id>http://uegeek.com/171021-DL3-How2UsePy3InDocker.html</id>
    <published>2017-10-21T02:48:57.000Z</published>
    <updated>2017-11-14T02:51:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/dl-week0-title.png" alt=""></p><a id="more"></a> <p>​在 <a href="http://www.uegeek.com/170304-deeplearning-week0.html" target="_blank" rel="noopener">DeepLearning-1：神经网络和 Docker 入门</a> 一文中，我们介绍了 Docker 环境的安装和使用。</p><p>然后，00 遇到了 Python 的经典问题：Python 2 还是 Python 3？TensorFlow image 默认安装的是 Python2，如果想在 Jupyter Notebook 里使用 Python3，怎么办呢？</p><p>在 <a href="https://github.com/tensorflow/tensorflow/issues/10179" target="_blank" rel="noopener">TensorFlow 的 这个 Issue</a> 可以看到，2017年5月已经支持<a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/" target="_blank" rel="noopener">用 tag 提供不同的 image</a>。比如 <code>tensorflow/tensorflow:latest-py3</code> 就可以（安装并）打开 Python3 环境。</p><p>结合目录映射的需要，输入命令完成映射并在 python3 环境下打开：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 8888:8888 -v ~/WorkStation/DeepLearning101-002/:/WorkStation/DeepLearning101-002 -w /WorkStation/DeepLearning101-002 tensorflow/tensorflow:latest-py3</span><br></pre></td></tr></table></figure><p>然后用<code>docker ps -a</code>查看所有 image，然后使用命令 <code>docker rename CONTAINER ID XXX</code>，将默认的 Python2 的 image 重命名为 dl，将 Python3 的 image 重命名为 dlpy3：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS                      PORTS               NAMES</span><br><span class="line">f46533729239        tensorflow/tensorflow:latest-py3   <span class="string">"/run_jupyter.sh -..."</span>   11 minutes ago      Exited (0) 6 minutes ago                        dlpy3</span><br><span class="line">f7178713446b        tensorflow/tensorflow              <span class="string">"/run_jupyter.sh -..."</span>   42 minutes ago      Exited (0) 15 minutes ago                       dl</span><br></pre></td></tr></table></figure><p>以后就可以根据需要，打开不同 Python 环境的 image。</p><p><code>docker start -i dl</code> 打开 Python2 环境：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/docker_py2.png" alt=""></p><p><code>docker start -i dlpy3</code> 打开 Python3 环境：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/docker_py3.png" alt=""></p><p>参考</p><ul><li><a href="https://github.com/tensorflow/tensorflow/issues/3467" target="_blank" rel="noopener">Docker Image with Python 3? · Issue #3467 · tensorflow/tensorflow</a></li><li><a href="https://github.com/tensorflow/tensorflow/issues/10179" target="_blank" rel="noopener">Support python3 on Docker image tensorflow/tensorflow:latest · Issue #10179 · tensorflow/tensorflow</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/dl-week0-title.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Deep learning" scheme="http://uegeek.com/tags/Deep-learning/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="Docker" scheme="http://uegeek.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>菜鸟数据科学入门03 - NumPy 数组基础和基本操作</title>
    <link href="http://uegeek.com/170929-DSNote3-NumPy-basic.html"/>
    <id>http://uegeek.com/170929-DSNote3-NumPy-basic.html</id>
    <published>2017-09-29T02:36:45.000Z</published>
    <updated>2017-11-14T02:44:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/data_science-title.jpg" alt=""></p><a id="more"></a> <h3 id="为什么用-NumPy？"><a href="#为什么用-NumPy？" class="headerlink" title="为什么用 NumPy？"></a>为什么用 NumPy？</h3><p><a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy</a> 是一个用于科学计算的基础 Python 库（<a href="http://www.scipy.org/scipylib/download.html" target="_blank" rel="noopener">安装说明</a>）。它可以让你在 Python 中使用向量和数学矩阵，以及许多用 C 语言实现的底层函数。</p><ul><li><p>简洁优雅</p><p>  当下大部分数据的组织结构是向量、矩阵或多维数组，NumPy 最重要的一个特点是 N 维数组对象（ndarray）。</p></li><li><p>效率高</p><p>  方便地计算一组数值，而不用写复杂的循环。</p></li><li><p>灵活兼容</p><p>  除了擅长科学计算，NumPy 还可以用作通用数据多维容器，可无缝对接各种各样的数据库。</p></li><li><p>敲门砖</p><p>  在数据科学中，有效的存储和操作数据是基础能力。如果想通过 Python 学习数据科学或者机器学习，就必须学习 NumPy。</p></li></ul><p>在 Notebook 中导入 NumPy：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><h3 id="什么是数组"><a href="#什么是数组" class="headerlink" title="什么是数组"></a>什么是数组</h3><p>数组是将数据组织成若干个维度的数据块。</p><blockquote><p>Array : data about relationships</p></blockquote><ul><li>一维数组是向量(Vectors)，由一个整数索引有序元素序列。</li><li>二维数组是矩阵(Matrics)，用一对整数（行索引和列索引）索引元素。</li><li>N 维数组(Arrays)是一组由 n 个整数的元组进行索引的、<strong>具有相同数据类型</strong>的元素集合。</li></ul><p><img src="http://image.slidesharecdn.com/2013-11-14-20enterthematrix-131207071455-phpapp02/95/enter-the-matrix-10-638.jpg?cb=1386400624" alt=""></p><h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><p>NumPy 的核心是数组（arrays）。</p><p>用 <code>array</code> 创建数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In[]: np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">Out[]: array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>在 NumPy 数组中，数据类型需要一致，否则，会尝试「向上兼容」，比如生成一个包含浮点数的数组，输出时每个元素都变成了浮点型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In[]: np.array([<span class="number">3.14</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">Out[]: array([ <span class="number">3.14</span>,  <span class="number">4.</span>  ,  <span class="number">2.</span>  ,  <span class="number">3.</span>  ])</span><br></pre></td></tr></table></figure><p>NumPy 还可以用循环生成数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In[]: np.array([range(i, i + <span class="number">3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">Out[]: array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure><p>用 <code>full</code> 生成一个 3 行 5 列的数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In[]: np.full((<span class="number">3</span>, <span class="number">5</span>), <span class="number">3.14</span>)</span><br><span class="line"></span><br><span class="line">Out[]: array([[ <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>],</span><br><span class="line">        [ <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>],</span><br><span class="line">         [ <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>,  <span class="number">3.14</span>]])</span><br></pre></td></tr></table></figure><p>用 <code>arange</code> 等距填充数组：</p><p>（arange 是 Python 内置函数 range 的数组版，返回的是一个 ndarray 而不是 list）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Starting at 0, ending at 20, stepping by 2</span></span><br><span class="line"></span><br><span class="line">In[]: np.arange(<span class="number">0</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Out[]: array([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">6</span>,  <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>])</span><br></pre></td></tr></table></figure><p>用 <code>linspace</code> 线性填充数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an array of five values evenly spaced between 0 and 1</span></span><br><span class="line"></span><br><span class="line">In[]: np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">Out[]: array([ <span class="number">0.</span>  ,  <span class="number">0.25</span>,  <span class="number">0.5</span> ,  <span class="number">0.75</span>,  <span class="number">1.</span>  ])</span><br></pre></td></tr></table></figure><p>用 <code>random</code> 生成随机数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a 3x3 array of random integers in the interval [0, 10)</span></span><br><span class="line"></span><br><span class="line">In[]: np.random.randint(<span class="number">0</span>, <span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">Out[]: array([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">          [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>btw 数组索引从 0 开始</p><p><img src="https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/httpatomoreillycomsourceoreillyimages1346880.png" alt=""></p><h3 id="数组切片"><a href="#数组切片" class="headerlink" title="数组切片"></a>数组切片</h3><p>NumPy 中的切片语法：<code>x[start:stop:step]</code>，如果没有赋值，默认值 start=0, stop=size of dimension, step=1。</p><p><img src="https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/httpatomoreillycomsourceoreillyimages1346882.png" alt=""></p><p>(上图最后一个图形，arr[1, :2]   应该是  (1,2) 一行二列矩阵？？）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">In[]: x[::<span class="number">2</span>]  <span class="comment"># every other element</span></span><br><span class="line"></span><br><span class="line">Out[]:array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">12</span>,  <span class="number">5</span>,  <span class="number">2</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">7</span>,  <span class="number">6</span>,  <span class="number">8</span>,  <span class="number">8</span>],</span><br><span class="line">       [ <span class="number">1</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">7</span>]])</span><br><span class="line">       </span><br><span class="line">In[]: x2[:<span class="number">3</span>, ::<span class="number">2</span>]  <span class="comment"># all rows, every other column</span></span><br><span class="line"></span><br><span class="line">Out[]:array([[<span class="number">12</span>,  <span class="number">2</span>],</span><br><span class="line">       [ <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">       [ <span class="number">1</span>,  <span class="number">7</span>]])</span><br></pre></td></tr></table></figure><p>复制数组切片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x2 = array([[<span class="number">99</span>  <span class="number">5</span>  <span class="number">2</span>  <span class="number">4</span>]</span><br><span class="line">    [ <span class="number">7</span>  <span class="number">6</span>  <span class="number">8</span>  <span class="number">8</span>]</span><br><span class="line">    [ <span class="number">1</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">7</span>])</span><br><span class="line">       </span><br><span class="line">In[]: x2_sub_copy = x2[:<span class="number">2</span>, :<span class="number">2</span>].copy()</span><br><span class="line">  print(x2_sub_copy)</span><br><span class="line"></span><br><span class="line">Out[]:[[<span class="number">99</span>  <span class="number">5</span>]</span><br><span class="line">   [ <span class="number">7</span>  <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><h3 id="数组转置和轴对换"><a href="#数组转置和轴对换" class="headerlink" title="数组转置和轴对换"></a>数组转置和轴对换</h3><p>reshape:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In[]: arr = np.arange(<span class="number">15</span>).reshape((<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">  arr</span><br><span class="line">  </span><br><span class="line">Out[]: array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">          [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br></pre></td></tr></table></figure><p>转置（transpose）是重塑（reshape）的一种特殊形式，返回源数据的视图而不进行复制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In[]: arr.T</span><br><span class="line"></span><br><span class="line">Out[]: array([[ <span class="number">0</span>,  <span class="number">5</span>, <span class="number">10</span>],</span><br><span class="line">       [ <span class="number">1</span>,  <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">       [ <span class="number">2</span>,  <span class="number">7</span>, <span class="number">12</span>],</span><br><span class="line">       [ <span class="number">3</span>,  <span class="number">8</span>, <span class="number">13</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">9</span>, <span class="number">14</span>]])</span><br></pre></td></tr></table></figure><h3 id="连接和拆分数组"><a href="#连接和拆分数组" class="headerlink" title="连接和拆分数组"></a>连接和拆分数组</h3><p>用<code>concatenate</code>连接数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In[]: grid = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                    [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">  np.concatenate([grid, grid])</span><br><span class="line"></span><br><span class="line">Out[]: array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># concatenate along the second axis (zero-indexed)</span></span><br><span class="line"></span><br><span class="line">In[]: np.concatenate([grid, grid], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Out[]: array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><p>用  <code>vstack</code>合并到数据行， <code>hstack</code> 合并到数据列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In[]: x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  grid = np.array([[<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">                 [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># vertically stack the arrays</span></span><br><span class="line">  np.vstack([x, grid])</span><br><span class="line"></span><br><span class="line">Out[]:array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><p>拆分数组的函数包括： <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html" target="_blank" rel="noopener">np.split</a>, np.hsplit, np.vsplit</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In[]: x = np.arange(<span class="number">8.0</span>)</span><br><span class="line">  np.split(x, [<span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">Out[]:  [array([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>]),</span><br><span class="line"> array([ <span class="number">3.</span>,  <span class="number">4.</span>]),</span><br><span class="line"> array([ <span class="number">5.</span>]),</span><br><span class="line"> array([ <span class="number">6.</span>,  <span class="number">7.</span>]),</span><br><span class="line"> array([], dtype=float64)]</span><br></pre></td></tr></table></figure><h3 id="使用-mask-快速截取数据"><a href="#使用-mask-快速截取数据" class="headerlink" title="使用 mask 快速截取数据"></a>使用 <code>mask</code> 快速截取数据</h3><p>传递给数组一个与它有关的条件式，然后它就会返回给定条件下为真的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In[]: norm10 = np.random.normal(<span class="number">10</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">  mask = norm10 &gt; <span class="number">9</span></span><br><span class="line">  mask</span><br><span class="line"></span><br><span class="line">Out[]:array([<span class="keyword">False</span>,  <span class="keyword">True</span>, <span class="keyword">False</span>,  <span class="keyword">True</span>, <span class="keyword">False</span>], dtype=bool)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In[]: print(<span class="string">'Values above 9:'</span>, norm10[mask])</span><br><span class="line"></span><br><span class="line">Out[]: (<span class="string">'Values above 9:'</span>, array([ <span class="number">13.69383139</span>,  <span class="number">13.49584954</span>]))</span><br></pre></td></tr></table></figure><p>在生成图形时也非常好用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">a = np.linspace(<span class="number">0</span>, <span class="number">2</span> * np.pi, <span class="number">50</span>)</span><br><span class="line">b = np.sin(a)</span><br><span class="line">plt.plot(a,b)</span><br><span class="line">mask = b &gt;= <span class="number">0</span></span><br><span class="line">plt.plot(a[mask], b[mask], <span class="string">'bo'</span>)</span><br><span class="line">mask = (b &gt;= <span class="number">0</span>) &amp; (a &lt;= np.pi / <span class="number">2</span>)</span><br><span class="line">plt.plot(a[mask], b[mask], <span class="string">'go'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/wHm77PYlbWAFAAAAABJRU5ErkJggg==.png" alt=""></p><p>在程序中用条件式选择了图中不同的点。蓝色的点（也包含图中的绿点，只是绿点覆盖了蓝点），显示的是值大于零的点。绿点显示的是值大于 0 小于 Pi / 2 的点。</p><h3 id="广播-Broadcasting"><a href="#广播-Broadcasting" class="headerlink" title="广播 Broadcasting"></a>广播 Broadcasting</h3><p>当不同 shape 的数组进行运算(按位加/按位减的运算，而不是矩阵乘法的运算)时，(某个维度上)小的数组就会沿着（同一维度上）大的数组自动填充。广播虽然是一个不错的偷懒办法，但是效率不高、降低运算速度通常也为人诟病。</p><blockquote><p>The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.<br>via <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#module-numpy.doc.broadcasting" target="_blank" rel="noopener">Broadcasting — NumPy v1.13 Manual</a></p></blockquote><p>广播的原理（via <a href="http://www.astroml.org/book_figures/appendix/fig_broadcast_visual.html" target="_blank" rel="noopener">Broadcast Visualization</a>）：</p><p><img src="http://www.astroml.org/_images/fig_broadcast_visual_1.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy.org</a></li><li><a href="http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" target="_blank" rel="noopener">Python Data Science Handbook</a></li><li><a href="https://book.douban.com/subject/25779298/" target="_blank" rel="noopener">利用Python进行数据分析</a></li><li><a href="http://www.scipy-lectures.org/index.html" target="_blank" rel="noopener">Scipy lecture notes</a></li><li><a href="http://www.slideshare.net/mikeranderson/2013-1114-enter-thematrix" target="_blank" rel="noopener">Enter The Matrix</a></li><li><a href="http://codingpy.com/article/an-introduction-to-numpy/" target="_blank" rel="noopener">使用 Python 进行科学计算：NumPy入门</a></li><li><a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#module-numpy.doc.broadcasting" target="_blank" rel="noopener">Broadcasting — NumPy v1.13 Manual</a></li><li><a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc" target="_blank" rel="noopener">EricsBroadcastingDoc - SciPy wiki dump</a></li><li><a href="http://www.astroml.org/book_figures/appendix/fig_broadcast_visual.html" target="_blank" rel="noopener">Broadcast Visualization — astroML 0.2 documentation</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/data_science-title.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Python" scheme="http://uegeek.com/tags/Python/"/>
    
      <category term="Data Science" scheme="http://uegeek.com/tags/Data-Science/"/>
    
      <category term="NumPy" scheme="http://uegeek.com/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>识别自动思维 —— HackYourself 避税手册之二</title>
    <link href="http://uegeek.com/170910-hackyourself-cbt2.html"/>
    <id>http://uegeek.com/170910-hackyourself-cbt2.html</id>
    <published>2017-09-10T09:47:59.000Z</published>
    <updated>2017-09-10T09:51:01.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/2-title.jpg" alt=""></p><a id="more"></a> <p>好久不见。</p><p><a href="http://www.uegeek.com/170624-hackyourself-cbt1.html" target="_blank" rel="noopener">HackYourself 避税手册第一篇</a>中，提出了「信念税」这个话题。怎样更好地「避税」呢？今天我们来聊聊如何识别自动思维。</p><p>当你在读这篇文章时，一部分注意力会集中在阅读文字并且理解、整合信息上面。同时，你可能会产生一些快速的评价，这些思维可以称为「自动思维」。它们似乎是立即自动涌现的，通常迅速而简短，一闪而过。我们往往很难觉察到这些想法的存在，更不要说进行理性、客观的评估，所以大都会顺着它们做出惯常的情绪和行为反应——就像飞行器进入「自动导航」模式一样。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/2-1autopilot.png" alt=""></p><p>「自动思维」的形成，大都不经过意识的「审查」，一旦形成却又根深蒂固，潜移默化地影响着我们的判断、选择、行为。</p><p>请想象一下，现在有一个小球，要滚过沙坑。</p><p><img src="https://ak2.picdn.net/shutterstock/videos/23753233/thumb/1.jpg" alt=""></p><p>第一次，沙坑上没有任何痕迹，小球可能可以沿很多路径滚过。</p><p>第二次让小球滚入沙坑，很有可能会顺着上一次的轨迹，并且进一步加深轨迹的深度。</p><p>反复几次，小球已经不太可能脱离轨迹滚动，而且一次比一次速度要快。</p><p>这就是路径依赖。</p><p><img src="http://amr.aom.org/content/34/4/689/F1.large.jpg" alt=""></p><p>自动思维的「路径」一旦形成，大脑就会沿着阻力最小的路径「思考」，完全跳过原本可能需要的觉察、思考、审视阶段，直接作出习性反应。</p><h3 id="回到自动思维起点，重新选择解释风格"><a href="#回到自动思维起点，重新选择解释风格" class="headerlink" title="回到自动思维起点，重新选择解释风格"></a>回到自动思维起点，重新选择解释风格</h3><p>自动思维都是一些不再经过意识「审查」的想法。想要改变那些已经形成固定路径的想法「小球」的运动轨迹，必须先回到原点，在小球启动的时候，重新获得对它有意识的控制。</p><p>试想一个我们经常会遇到的情景：工作上有一件事情没有做好。</p><p>面对这个结果时，我们可能会产生很多想法。这时候「小球」其实有很多不同的路径可以选择：</p><table><thead><tr><th>解释风格</th><th>归因</th><th>态度/行为导向</th></tr></thead><tbody><tr><td>具体的</td><td>我第一次做，经验不足，目标设定也有些过高，超出了我掌握的技能</td><td>继续学习和练习</td></tr><tr><td>笼统的</td><td>我没有能力做好</td><td>我真是个差劲的人</td></tr><tr><td>短暂的</td><td>最近身体不好影响了状态</td><td>恢复身体迎头赶上</td></tr><tr><td>长期的</td><td>我不是那种有信心、什么都能做好的人，一直都是这样</td><td>以后不要尝试算了</td></tr><tr><td>个人的</td><td>这个项目虽然不成功，我还是锻炼了 xxx 能力</td><td>想想这次的经验怎么复用</td></tr><tr><td>普遍的</td><td>这件事没有什么意义，一开始我就不看好</td><td>真是浪费时间浪费表情</td></tr></tbody></table><p>从上表可以看到，如果对负面结果选择具体、短暂、普遍的解释风格，而不是笼统、长期的和个人的方式来解释，可以减少挫折和无助。</p><p><strong>乐观的人一般会用长期的、笼统的、个人的方式解释成功，而用短暂的、具体的、普遍的方式解释失败。</strong>如果事事要求只做最好，就会抑郁：无法达到的预期，加上为失败承担个人责任的倾向，形成了致命的组合。</p><h3 id="如何识别不恰当的自动思维"><a href="#如何识别不恰当的自动思维" class="headerlink" title="如何识别不恰当的自动思维"></a>如何识别不恰当的自动思维</h3><p>我们每天会产生数量难以估算的自动思维，几乎每时每刻都有自动思维在进行，不可能也没有必要全部识别出来。那么，哪些自动思维值得识别？如何判断是否「恰当」？</p><p>让我们先聚焦在那些引起较明显情绪反应、生理变化、行为变化的时刻，以及可能做出重要决策的时刻，这些时刻的自动思维较容易识别，也比较可能带来较大的影响。</p><p>情绪是一个非常好的线索，它提醒我们「此刻也许该暂停」。</p><p><img src="https://cdn.dribbble.com/users/552894/screenshots/3716113/y3ll.jpg" alt=""></p><p>当情绪上涌时，不妨试着做三件事情：</p><h4 id="1-暂停"><a href="#1-暂停" class="headerlink" title="1.暂停"></a>1.暂停</h4><p>比如说，Boss 刚刚布置了一项很有挑战的任务，让你在 3 天内完成。这时身体会不自觉地开始有一些反应，比如头皮发麻，脑子一片空白，内心有一群草原动物奔驰而过……</p><p>在这样一个情绪节点，请尽可能先按下暂停键，把时空「冻结」住。</p><p><img src="https://cdn.dribbble.com/users/408943/screenshots/2941267/play-pause.gif" alt=""></p><h4 id="2-识别情绪"><a href="#2-识别情绪" class="headerlink" title="2. 识别情绪"></a>2. 识别情绪</h4><p>观察一下自己的第一反应：</p><blockquote><p>刚才我产生了什么反应和情绪？</p></blockquote><p>身体有什么反应？是胸口发紧、太阳穴微微发热、眉头开始皱起？情绪有什么变化？是焦虑——觉得时间太紧不知道该从何入手，是不满——这么难的事情给到自己不公平有一点愤怒，还是恐惧——害怕把事情搞砸，别人会对自己有负面评价？</p><h4 id="3-识别自动思维"><a href="#3-识别自动思维" class="headerlink" title="3. 识别自动思维"></a>3. 识别自动思维</h4><p>顺着情绪反应，我们也许可以抓出引起这些情绪的「自动思维」：</p><blockquote><p>刚才我脑子里闪过什么念头？</p></blockquote><p>比如</p><ul><li>这太难了</li><li>我可能没办法做到</li><li>如果我试了但是没有用怎么办？</li><li>我很想拒绝但是担心拒绝会带来严重后果</li></ul><p>当暂停成功后，识别出当下的想法，就容易产生 second thought。如果可以产生更多的想法，而且别急着站队，就可以让想法之间相互 pk，那些不那么合理的想法就会现出原形，让我们更容易做出恰当的评估。</p><p><img src="https://cdn.dribbble.com/users/86429/screenshots/416545/thoughtbender_sounas.png" alt=""></p><p>比如说「这件事情太难，我做不了」，在这个节点，我们通过「冻结」时空，重新获得挑选解释的主动权。以前，我们可能会下意识地选择逃避，但是如果转念一想，意识到「这个项目是锻炼组织能力的好机会」，我们就可能更愿意接受挑战；如果想到「这个项目我一个人不行，但是如果 xx 能加入，我会更有信心」，我们就可能会开始考虑如何申请资源……</p><h3 id="识别练习"><a href="#识别练习" class="headerlink" title="识别练习"></a>识别练习</h3><p>当然，整个过程需要反复练习，毕竟我们对抗的是已经千百次滚过沙坑的「自动导航」思维小球。</p><p>怎样更好地练习呢？</p><h4 id="1-提高情绪的分辨率"><a href="#1-提高情绪的分辨率" class="headerlink" title="1.提高情绪的分辨率"></a>1.提高情绪的分辨率</h4><p>如果我们只能区分出「好」和「坏」情绪，情绪感知的分辨率过低，也就很难通过情绪觉察自己的状态，从而追溯出产生情绪的自动思维。</p><p>语言会限制思维。提高情绪分辨率的第一步，可以尝试扩充关于情绪的词汇和定义。</p><p><img src="https://img3.doubanio.com/lpic/s28948495.jpg" alt=""></p><p><a href="https://book.douban.com/subject/26849228/" target="_blank" rel="noopener">「心情词典」</a>一书中介绍了 154 种情绪，例如：</p><table><thead><tr><th>A</th><th>B</th><th>C</th></tr></thead><tbody><tr><td>ABHIMAN 由爱生恨</td><td>BAFFLEMENT 不知所措</td><td>CALM 冷静</td></tr><tr><td>ACEDIA 倦怠</td><td>BASOREXIA 亲吻渴望</td><td>CAREFREE 无忧无虑</td></tr><tr><td>AMAE 撒娇依赖</td><td>BEFUDOLEMENT 茫然</td><td>CHEERFULNESS 欢快</td></tr><tr><td>AMBIGUPHOBIA 非解释清楚不可</td><td>BEWILDERMENT 困惑</td><td>CHEESED 气恼</td></tr><tr><td>ANGER 愤怒</td><td>BOREDOM 厌倦</td><td>CLAUSTROPHOBIA 幽闭恐惧</td></tr><tr><td>ANTICIPATION 期待</td><td>BRABANT 作死</td><td>COLLYWOBBLES 肠胃焦虑</td></tr><tr><td>ANXIETY 焦虑</td><td>BROODINESS 求子心切</td><td>COMFORT 安慰</td></tr><tr><td>APATHY 冷漠</td><td></td><td>COMPASSION 同情</td></tr><tr><td>L’APPEL DU VIDE 虚空的呼唤</td><td></td><td>COMPERSION 多元之爱</td></tr><tr><td>AWUMBUK 人去心空</td><td></td><td>CONFIDENCE 自信</td></tr></tbody></table><p>具体的情绪解读，可以翻一翻这本书。</p><p>当我们掌握更多细致描述情绪的词汇，对情绪的感知和识别能力也会得到提升。</p><h4 id="2-将自动思维形象化"><a href="#2-将自动思维形象化" class="headerlink" title="2.将自动思维形象化"></a>2.将自动思维形象化</h4><p>尝试给想要调整的自动思维塑造一个生动的形象，比如，纸老虎、聒噪的青蛙、严厉的老师、无脸人之类的，<strong>诀窍是选择那种看起来很强大，实际上容易怂的形象</strong>，嘿嘿嘿。。。每次识觉察到自动思维又在默默推动小球滚过沙池，我们就可以尝试跟这个形象开始对话。</p><p>经过一些练习，你也许会发现这个形象越来越具体，也就越容易被识别出来，自己就不容易受它所代表的自动思维所控制。</p><h4 id="3-写情绪日记"><a href="#3-写情绪日记" class="headerlink" title="3.写情绪日记"></a>3.写情绪日记</h4><p>很多时候，自动思维的惯性太大，不一定能够在发生的当下即时识别。于是有必要定期做一些回顾和反思。比如尝试写情绪日记。每天晚上花 5 分钟，想想过去的一天，自己经历了哪些情绪强烈的时刻？当时涌现了什么样的自动思维？下次再出现，如何更快识别出来？识别后如何应对？需要什么帮助和工具吗？……</p><p>我们也可以尝试将自己经常会出现的（负面）自动思维列一个清单，作为「信念税单」，每天检查一下，今天有没有为这些信念「交税」。</p><p><img src="https://cdn.dribbble.com/users/153946/screenshots/1089494/clipboard.png" alt=""></p><p>现在就开始切换自动导航模式的练习吧！</p><p>你有哪些常见的「自动导航思维」？识别它们的过程中，有什么感想和心得想跟大家分享？欢迎留言！</p><p>祝各位避税顺利！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/2-title.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="Psychology" scheme="http://uegeek.com/tags/Psychology/"/>
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="Mind" scheme="http://uegeek.com/tags/Mind/"/>
    
  </entry>
  
  <entry>
    <title>Time will tell</title>
    <link href="http://uegeek.com/170723-time-will-tell-md.html"/>
    <id>http://uegeek.com/170723-time-will-tell-md.html</id>
    <published>2017-07-23T09:45:33.000Z</published>
    <updated>2017-09-10T09:48:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.dribbble.com/users/89889/screenshots/3658659/toolittletime-dribbble.gif" alt=""></p><a id="more"></a> <p>昨天，跟咨询师一起整理了个人的近况和困扰，收获颇多。</p><p>很多时候，我们会因为一时失去焦点，模糊了初心、价值观和评判标准，而走入一座霾雾缭绕的迷宫中。什么是对的？什么应该坚持？哪些需要改变？虽然表象看起来纷繁复杂，但只要不失初心，气沉丹田，前路自然会显出轮廓。</p><p>今天继续反思最近的工作和生活，再次意识到，做生意和过生活的好多问题和解法，是一样一样的。</p><h3 id="商业的本质和初心"><a href="#商业的本质和初心" class="headerlink" title="商业的本质和初心"></a>商业的本质和初心</h3><p>怎样判断一个公司是好公司？大概，遵循商业规律，能持续满足市场需求，就是一家好公司。那么，什么样的公司是值得尊敬的公司？</p><p>之前跟朋友讨论过，创业成功的标准是什么？我认为是：</p><blockquote><p>「 人 心 所 向 」</p></blockquote><p>不仅仅要让客户满意，而且让几乎所有的利益相关者比如员工、同行、行业上下游，甚至包括竞争对手，都愿意支持你、拥护你，这就是战无不胜的「人心所向」。所以，马云在阿里巴巴一直强调：客户第一，员工第二，股东第三。因为，商业运作的基本逻辑，就是生产出巨大的价值（价值一定是面向某些主体的），然后从这些价值中获取其中的一小部分作为企业利益和发展资源。</p><p>于是，从外部来看，我们知道一个公司的目标是「造势」：通过组织资源、提供价值，做到「人心所向，大势所趋」。</p><p>那么，从内部看，怎样判断一个公司运作良好？</p><p>我认为是看熵增还是熵减。</p><blockquote><p>一个生命有机体在不断地产生熵，并逐渐趋近于最大熵的危险状态，即死亡。要摆脱死亡，要活着，唯一的办法就是从环境不断汲取负熵。一个生命有机体具有推迟趋向热力学平衡（死亡）的奇妙能力，就像是活有机体吸引一串负熵去抵消它在生活中产生的熵总量，从而使它自身维持在一个稳定而又低熵的水平上。 —— 薛定谔 「生命是什么」</p></blockquote><p>类比生命有机体，一个创业公司如果还有活力，那么它应该会有明确的创业动机或愿景，还在不断的运动拓展，同时有强烈的不死意愿。</p><p>存活的目的是进一步生长。生长是熵减的过程。对公司而言，首先需要一个稳定而有序的团队结构，然后需要找到能够复制和 scale 的业务模式。如果想长期拥有竞争力，还必须有一套有序犯错的多元化基因库。初创公司能够在竞争激励的商业环境中存活，靠的是差异化生存。而差异来自包容和创新。</p><p>简单来说，就是看一个公司有没有向着更有序的趋势发展。</p><p>不过，也有例外——公司发展最迅速的阶段，往往最混乱。那句话说得好（原谅我忘记出处了）：</p><blockquote><p>什么是健康？健康就是要病不病的边缘状态。</p></blockquote><p>当公司飞速发展的时候，可能内部看起来无比混乱，但是却使外部产生熵减，比如市场份额快速扩大、口碑扩散、替代传统技术和方式等等。也就是说，为了在外部快速获得熵减的「势」，可以一定程度容忍内部的熵增。当然，不要以为所有的混乱无序，都反映了业务快速增长，它可能只是生命体衰竭的先兆。</p><p><strong>外部人心所向，内部有序熵减</strong>，可以作为判断一个公司健康状况的标准，也是一个公司应该努力的目标——而不是仅仅盯着财务数据（虽然能更直接反映短期健康状况）。</p><p>于是又冒出另一个问题：我们要在多长的时间周期内，做好创业这件事？做一家 2 年内的风口网红公司，跟做一家 20 年后依然存活而且受人尊敬的公司，策略完全不一样。把事情放在更大的时间周期内来考虑，格局自然大不同。而评估标准就更简单了：较大的时间周期内，目标利益群体会不会用脚投票（比如客户来买单、人才来加入、同行来合作）。</p><p>于是再冒出另外一个问题：做一家能存活的公司，做一家好公司，做一家受人尊敬的公司，它们之间有先后次序吗？按常理来看，应该先生存，后发展。但是换个角度想，一开始就做受人尊敬的事，兢兢业业地为客户提供更大的价值，这些事难道不会带来商业利益吗？</p><p>创业和工作，还是要回到初心。</p><p>凡是真正值得做的事情，就值得慢慢做。因为啊，慢慢来，比较快。</p><h3 id="个人的期许与方向"><a href="#个人的期许与方向" class="headerlink" title="个人的期许与方向"></a>个人的期许与方向</h3><p>回到每一个个体，迷茫时怎么判断自己是不是走在所谓「正确」的道路上？上面的逻辑依然好用：</p><ul><li>外部人心所向：我是否获得持有类似价值观的群体的认可？</li><li>内部有序熵减：我是否获得可以增值的能力，比如洞察力，学习能力，分析和解决问题的能力，创造和表达能力，等等</li></ul><p>对公司而言，跟目标利益群体保持长久持续的关系，可以作为评估标准。商业逻辑，天下大同。但是个体却千人千面。对个人而言，可参考的标准是什么？</p><p>好像只能基于每个人自己的价值观和偏好，来寻找这个所谓的「标准」。</p><p>对我而言，应该是能创作出或美好或有趣的作品吧。然后，用自己的作品去吸引那些价值观近似的人，与他们产生紧密的连接。</p><p>我讨厌认知负荷——所以做设计真的很容易开心。洁癖这个词太简单粗暴，他们大概是一个对「秩序」很敏感的人群。认知负荷也是一种熵增，思维混乱是熵增，作息不规律是熵增，做事毫无目的性是熵增……</p><p>「有序和无序」大概是我这辈子的人生课题。我希望那些没有什么美感的东西能尽量简单，甚至成为 API 接口，好让我们节省精力，用在更需要创造力的地方——用于创造那些或有序或无序的美的过程中。</p><p>对我而言，判断一个人是不是同路人，看看 ta 有多喜欢美好的事物、有没有创造出尽可能美好的事物就知道了 —— 美好的事物，都是通过艰辛努力获得的熵减的成果。</p><p>什么东西足够「美好」呢？要我说啊，那些不符合最小化生存成本、自身脆弱却又能抵御时间侵袭的东西，大抵都会是相当美好的东西。</p><p>时间是最好的判官，虽然有的公司能借着风口起飞，但是百年老店才是影响几代人的生意。</p><p>时间是最好的判官，在个体能够感受的范围内，最长也最公正的尺度是一生。一辈子里面，真正给自己留下记忆，给别人带来益处的事情，都会有哪些。当然，有的人还能影响和帮助往后数千年的人，这种价值无法估量，弥足珍贵。</p><p>当我们纠结、焦虑的时候，不妨试着抽出三条时间线：</p><p>第一条，叫做「生命长河」。</p><p>在我们自己的生命之河中，眼前都只是一个切片。一天的事放在一个月的周期来看，大概就不算什么。一时的事放在十年的周期来看，并没有我们想象得那么重要。真正重要的，是做这件事的姿势，该不该保持十年，值不值得用十年或更长的时间去保持。</p><p>第二条，叫做「迈尔斯通」（Milestone -.-）。</p><p>如果只看到长河的终点，可能容易因为路途遥远而丧失了勇气。马拉松的冠军们，大都不以终点为目标，而是会不停地设置看得见的标识物为目标：下一个路口、下一个补给站、下一个拐弯、下一个树荫……所谓日拱一卒，不期速成。棋盘才摆开，不要想着终局，先把下一个要吃掉的小兵盯好。</p><p>最后一条，叫做「刹那永恒」。</p><p>过去和未来都不可期，只有当下最真实。当下的一心一念，会影响一言一行，也会影响那个昵称可能叫做「薛定谔」的未来的模样。</p><p>让我们面对生命长河，聚焦在看得见的下一个界碑，全神贯注地走好现在这一步。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.dribbble.com/users/89889/screenshots/3658659/toolittletime-dribbble.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="Psychology" scheme="http://uegeek.com/tags/Psychology/"/>
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="创业" scheme="http://uegeek.com/tags/%E5%88%9B%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>算法作曲历险记01-简史</title>
    <link href="http://uegeek.com/170713-algorithmic-composition-1.html"/>
    <id>http://uegeek.com/170713-algorithmic-composition-1.html</id>
    <published>2017-07-13T01:03:19.000Z</published>
    <updated>2017-07-15T01:25:43.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/technocombat_ilu_800x600.jpg" alt=""></p><a id="more"></a> <h3 id="AC-和-AC"><a href="#AC-和-AC" class="headerlink" title="AC 和 AC"></a>AC 和 AC</h3><p>Art &amp; Code （AC）是 00 新开的坑，记录对一个陌生但好玩的领域的学习历程。算法作曲(Algorithmic Composition) 是 AC 中的 1 号 AC 坑……欢迎围观我在大雾中摸索、游荡。也欢迎老司机帮我带带路~</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/ACACAC-illustrate.jpg" alt=""></p><p>算法作曲 顾名思义，不是由人来作曲，而是经由设定好的算法，通过机器自动生成乐曲。</p><p>先别震惊，也别反感。我并不觉得机器会威胁人在音乐、艺术创造领域的优势。之所以研究算法作曲，纯粹出于对这两个领域的兴趣 —— 尤其是两个看似矛盾、对立的领域，碰撞在一起，一定有很嗨的火花吧！</p><blockquote><p>音乐虽然在表象上是流动音响的时间艺术，然而，从其基础理论设计与数学逻辑同构并进行符号化组织的角度来看，似乎没有任何一门艺术，天生能比音乐拥有更强的可计算性。 ​​​​<br>——黄雷基 <a href="https://mp.weixin.qq.com/s?__biz=MzA3Mjk0MTcyNg==&amp;mid=2649683495&amp;idx=1&amp;sn=18d9af0062b25b6f090bd291d2b4ffc7&amp;mpshare=1&amp;scene=1&amp;srcid=0624qKxE2leEZXU59q4k3vku&amp;key=51ed750aa8560dbf88c430bde4d013799282dbe39aca0f765f6ea94e75f43f04e7f3d2939f531de5d7e753b15c619173871930a2d3ce47bcf41660187643f6b1fe9b8aff79371fd8a771f3d1fa3693dd&amp;ascene=0&amp;uin=MTMyNjgw&amp;devicetype=iMac+MacBook8%2C1+OSX+OSX+10.12.5+build(16F73" target="_blank" rel="noopener">算法作曲的前世、今生和未来——回溯</a>)</p></blockquote><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/robot_rock_dribbble.jpg" alt=""></p><p>站在坑口大概往里看了一眼，真是深不见底啊！基础乐理得学吧，作曲知识得学吧， DSP 得略懂一些吧，算法大概得了解吧，机器学习/深度学习得看看吧，工具和软件得试着用吧，前沿的项目得研究下吧……</p><p>一把年纪了怎么这么想不开？大概是年纪大了，更加觉得应该多花些时间在（起码看起来）好玩的事情上面呢！</p><p>系列第一篇，一起来回顾一下算法作曲并不长的历史。</p><h3 id="算法作曲简史"><a href="#算法作曲简史" class="headerlink" title="算法作曲简史"></a>算法作曲简史</h3><p>1952 年，28 岁的列哈伦•希勒（Lejaren Hiller􏶃）告别了杜邦公司，怀揣一大笔奖金重返校园。在美国伊利诺伊大学，这位 23 岁就拿到博士学位的化学家，开始了对合成橡胶的研究。</p><p><img src="http://www.musicainformatica.it/wp-content/uploads/2013/06/Lejaren_Arthur_Hiller_composer_holding_score.jpg" alt=""></p><p>也正是在同一年，第一台完全由美国大学研发的冯•诺伊曼式计算机 Illiac 正式亮相伊利诺伊大学。</p><p><img src="http://2.bp.blogspot.com/-a8PLqfvNuLQ/UKHtOgivzRI/AAAAAAAAAE4/PYUGVS7mV68/s1600/illiac1.png" alt=""></p><p>得天独厚的希勒得以接触到电子计算机，并用它来计算统计意义上理想聚合分子的大小。他无意中发现，如果将控制变量由几何数变为音符，同样的计算机代码可以用于对位法谱曲。一向对音乐颇有兴趣的希勒开始不务正业，试着利用计算机来谱曲，他不仅在闲暇时间开始攻读音乐硕士学位，还把自己的助手——化学师里奥纳德•艾撒克逊（􏹴􏹵􏰉􏻺􏹛Leonard Isaacson）一同拉下了水。</p><p><img src="http://s7.computerhistory.org/is/image/CHM/500004127-03-01?$re-medium$" alt=""></p><p>1956年，在美国计算机协会（ACM）的一次会议上，希勒做了一场关于计算机创作音乐的报告。听众的反应不一，有趣的是，计算机专家们大多对此持非常开放的态度，音乐家们则显得更为谨慎，而很多其他领域的学者们却认为他完全是在胡说八道。</p><p>不管怎样，希勒的第一个作品在 1957 年诞生。</p><p>为了纪念用来作曲的计算机 Illiac，他为这支弦乐四重奏取名「依利亚克组曲」（Illiac Suite），这也是历史上第一支完全由计算机生成的音乐作品。他们基于产生——测试的方法创作，首先使用 Markov 链模型来产生有限控制的随机音符，之后利用和声与复调的规则测试这些音符，最后选择符合规则的材料，修改、组合成传统音乐记谱的弦乐四重奏。</p><p><img src="http://blogs.diariovasco.com/bigbang/files/IlliacSuiteexperimento1br.jpg" alt=""></p><p>【找乐曲】</p><p><a href="http://www.musicainformatica.org/topics/illiac-suite.php" target="_blank" rel="noopener">该作品分为四个乐章</a>：</p><ul><li>第一乐章：计算机生成的不同长度的固定主题旋律</li><li>第二乐章：使用变奏的规则生成的四声部音乐</li><li>第三乐章：通过计算机对节奏、动态和演奏法的不同处理生成的音乐</li><li>第四乐章：通过衍生算法和马尔科夫链的不同模型及概率生成的音乐</li></ul><p>希勒从此彻底投入计算机音乐创作的怀抱，他转入音乐系任教，成立实验性的音乐工作室。</p><p>随后希勒又开拓了他的很多个「第一次」，如最先进行乐谱打印，最早使用物理模型合成技术等，创作了「算法」系列，「计算机康塔塔」，「HPSCHD」等一系列计算机作曲的音乐作品。</p><p>从此之后，在计算机快速发展的黄金年代，算法作曲也在世界各地的领域先驱们推动下快速发展。</p><p>1960 年，俄罗斯研究者 R.Kh.Zaripov 发行了全世界第一篇关于用「Ural-1」计算机进行算法音乐作曲的论文 <a href="http://adsabs.harvard.edu/abs/1960SPhD....5..479Z" target="_blank" rel="noopener">An Algorithmic Description of a Process of Musical Composition</a>。</p><p>1960 年，法国工程师、作曲家 Pierre Barbaud 成为欧洲第一个使用计算机作曲的作曲家。</p><p><img src="http://www.associationpierrebarbaud.fr/barbaud_portrait_AClaass.jpg" alt=""></p><p>他创作了「7!」，在有限状态机 (Finite State Automata) 和随机矩阵 (Stochastic Matrices) 的帮助下，将一系列规则运用到 12 音体系中，以此为调性和序列音乐风格建模。</p><p>1965年，发明家 Ray Kurzweil 首次公演了一首用计算机来创作的钢琴作品，这台计算机能够对各种不同的乐曲进行模式识别，并且可以分析和使用这些模式去创造新的旋律。</p><p>1974年，第一次国际计算机音乐会议召开（ICMC）。</p><p>随着计算机相关技术的发展和普及，越来越多的人关注到这个科技与艺术奇妙结合的领域，于是在那个新生流派争奇斗艳的先锋实验年代，各式各样的作曲算法涌现出来。</p><p>然而直到美国作曲家大卫·科普(David  Cope) 开发的「音乐智能实验」系统(Experiments in Musical Intelligence，简称 EMI，1981 年开始研究) ，1987 年在国际计算机音乐年会上第一次公开展示初期成果那时起，计算机音乐与传统音乐之间的桥梁才逐渐架设起来。</p><p><img src="http://50years.ucsc.edu/css/assets/images/posts/fourth-decade/1995-david-cope.jpg" alt=""></p><p><img src="http://www.computerhistory.org/atchm/wp-content/uploads/2015/04/MI0000985908-BachByDesign.jpg" alt=""></p><p>Cope 最初只想编写一个能够描述自己作曲风格的计算机程序，并用来记录自己作品的发展轨迹，不过很快他将目标转向了那些已故大师们，希望能用软件创造出带有不同大师风格的音乐作品。该系统所采取的方法涉及音乐文法和建立风格数据库等， Cope 将这样的工作称为 recombinancy —— 在现存的音乐上加入新的、符合原有逻辑的演绎，从而创造出新的作品。</p><p>比如 David Cope 用 EMI 创作的维瓦尔第风格的十二首管弦乐作品：「星座」。下面将要听到的这首是 「金牛座」（视频也是用算法自动生成的）</p><div class="video-container"><iframe src="//www.youtube.com/embed/2kuY3BrmTfQ" frameborder="0" allowfullscreen></iframe></div><p>1993年，美国科学家 John Al Biles 运用交互式遗传算法，设计了名为 GenJam 的交互式创作演奏系统。该系统能在现场演奏者的 4 或 8 小节演奏后单独生成一个类似风格的新旋律，也能与演奏者进行合奏，在演奏者演奏的同时生成并演奏一个新旋律。</p><p><img src="http://2.bp.blogspot.com/-naDY6-Sc_hU/Upgx1HgAJMI/AAAAAAAAAHg/ihtsVmHgkp0/s1600/genjam.png" alt=""></p><p>随着计算机制谱、乐器数字接口、信号处理、声音分析等技术的研究和应用逐渐兴起和深入，计算机音乐已经成为一个跨学科、跨媒介、跨文化的艺术形式和科学领域。</p><p>这么好玩的坑，要入吗？</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>倪朝晖. (2015). 算法作曲理论与实践. 西南师范大学出版社</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3Mjk0MTcyNg==&amp;mid=2649683495&amp;idx=1&amp;sn=18d9af0062b25b6f090bd291d2b4ffc7&amp;mpshare=1&amp;scene=1&amp;srcid=0624qKxE2leEZXU59q4k3vku&amp;key=51ed750aa8560dbf88c430bde4d013799282dbe39aca0f765f6ea94e75f43f04e7f3d2939f531de5d7e753b15c619173871930a2d3ce47bcf41660187643f6b1fe9b8aff79371fd8a771f3d1fa3693dd&amp;ascene=0&amp;uin=MTMyNjgw&amp;devicetype=iMac+MacBook8%2C1+OSX+OSX+10.12.5+build(16F73" target="_blank" rel="noopener">算法作曲的前世、今生和未来——回溯</a>)</li><li><a href="http://www.musicainformatica.org/topics/illiac-suite.php" target="_blank" rel="noopener">Illiac Suite | musicainformatica.org</a></li><li>Hoffmann, P. (2009). Music Out of Nothing? A Rigorous Approach to Algorithmic Composition by Iannis Xenakis. <a href="https://doi.org/http://dx.doi.org/10.14279/depositonce-2292" target="_blank" rel="noopener">https://doi.org/http://dx.doi.org/10.14279/depositonce-2292</a></li><li><a href="https://ccrma.stanford.edu/~blackrse/algorithm.html" target="_blank" rel="noopener">The History of Algorithmic Composition</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/technocombat_ilu_800x600.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="AlgorithmicComposition" scheme="http://uegeek.com/tags/AlgorithmicComposition/"/>
    
      <category term="算法作曲" scheme="http://uegeek.com/tags/%E7%AE%97%E6%B3%95%E4%BD%9C%E6%9B%B2/"/>
    
      <category term="Art" scheme="http://uegeek.com/tags/Art/"/>
    
      <category term="Code" scheme="http://uegeek.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>警惕信念的苛捐杂税 —— HackYourself 避税指南之一</title>
    <link href="http://uegeek.com/170624-hackyourself-cbt1.html"/>
    <id>http://uegeek.com/170624-hackyourself-cbt1.html</id>
    <published>2017-06-24T01:03:19.000Z</published>
    <updated>2017-07-15T01:15:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/dailythoughts.png" alt=""></p><a id="more"></a> <p>人每天会产生多少想法？</p><p>别着急回答。</p><p>试着闭上眼睛 30 秒，什么都不干，「观察」一下脑子里会出现些什么念头。</p><p>准备好了，闭上眼睛。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/30secondscountdown.gif" alt=""></p><p>留意到自己的念头了吗？</p><p>大部分人会惊诧于这 30 秒内产生的念头竟然有那么多，而且杂乱无章。</p><p>没错，在醒着的大部分时间里，我们持续产生着各式各样的念头。据未确切考证的<a href="http://blogs.discovermagazine.com/neuroskeptic/2012/05/09/the-70000-thoughts-per-day-myth/#.WU2hihOGOHo" target="_blank" rel="noopener">说法</a> <sup>[1]</sup>，人一天中产生的念头多达六七万个。这些念头闪过，却基本不会觉察它们的存在。</p><p>想想看，假设你开了一家大公司，有六七万人给你打工，但是你根本不知道他们都从哪里来。幽灵一般的员工们快速地来回穿梭，看似在工作，实际上只是机械地做着动作，你搞不清他们在做什么、公司是如何运作的。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/assembly-800-x-600.gif" alt=""></p><p>够可怕吧？这恰恰就是我们的常态——每分每秒都在产生念头，看似我们在「制造」和「控制」它们，实际上却被它们「影响」然后「不自觉地行动」。而且，念头也有「阶级」之分。有些念头催生了其他念头，属于坐享权力和影响力的「源头信念」。</p><p>也许我们没有意识到，每天都在为一些念头（尤其是源头信念）付出巨大的代价。某些信念让我们反复做出类似的决策，导致类似的结果，甚至形成一再重复的模式，反复上演相似的历史。比如从来没法坚持做好一件事，比如每个月都刷爆信用卡，比如感情上一直遇到渣男……</p><p>如果总是把结果轻描淡写归结为「一时糊涂」、「运气不好」等等，可能没有想过、也不愿意承认，这些结果往往源于一些未经审视的信念。不会有任何提醒，也没有半张收据，就像税赋一样，吸血于无形之中，然而你每天每月每年都在交，甚至要交上一辈子。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/giveandtake.jpg" alt=""></p><p>时隔半年，HackYourself 回归的第一个系列，00 想跟大家聊聊信念的苛捐杂税——我们在为哪些念头付出沉重代价，如何合理避税以便早日实现「精神的财务自由」。</p><h3 id="小税可逃，大税难免"><a href="#小税可逃，大税难免" class="headerlink" title="小税可逃，大税难免"></a>小税可逃，大税难免</h3><p>想逃税也需要界定一个范围，毕竟小税可逃，大税难免。</p><p>什么是「大税」呢？比如属于时代的信念税——历史局限性，这可是连重构学科体系的大佬们都无法突破的的框架：</p><p><img src="https://image.slidesharecdn.com/earthyquotesrev-12824374135542-phpapp02/95/paradigms-14-728.jpg?cb=1282419576" alt=""></p><p>时代信念税，往往一交就交大发了，世界商业版图都从此改变：</p><p><img src="http://img.picturequotes.com/2/587/586733/i-think-there-is-a-world-market-for-maybe-five-computers-quote-1.jpg" alt=""></p><p>错过一个时代不说，还会被后人耻笑：</p><p><img src="https://lukasdaalder.files.wordpress.com/2014/02/screenshot-2014-02-11-06-33-04.png?w=440&amp;h=230&amp;crop=1" alt=""></p><p>如果说「大税」交的是历史周期的税，那么「小税」又包含什么呢？让我们从群体历史周期回到个体历史周期。</p><p>从童年开始，我们已经对自己、他人以及世界形成了一定的看法，其中一些最核心的信念被认为是绝对真实和正确的。这些影响个体价值观、判断和行为的观念，关于自我最核心的观念，是避税指南系列想要探讨的重点。</p><p>认知语言学家莱考夫在「我们赖以生存的隐喻」一书中，语重心长地指出，我们的概念体系基于隐喻构建，抽象思维很大程度上是隐喻的，我们通过隐喻派生的推论为基础来生活。这些隐喻如此根深蒂固，我们根本都察觉不出它们的深刻影响。比如，「争论是战争」这一隐喻，贯穿在日常对「争论/讨论/辩论」等概念的理解和行动中：</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/goodargument.jpg" alt=""></p><blockquote><p>他<strong>攻击</strong>我观点中的每一个弱点</p><p>我<strong>粉碎</strong>了他的论点</p><p>和他争论，我从来没<strong>赢</strong>过</p><p>你不同意？好吧，<strong>反击</strong>啊！</p><p>他<strong>击破</strong>了我的所有观点</p><p>……</p></blockquote><p>我们对每一句话都习以为常，用起来自然得根本意识不到争论也许有其他的形式和比喻，比如像跳双人舞，比如像打乒乓球。</p><p>我的一位朋友在婚姻中被婆媳关系折磨得发疯，一家人的无数冲突，可能都源于婆婆一直灌输给她丈夫的「信念」：一定要制服媳妇，让她听话、服服帖帖，不会骑在自己头上。多么经典的战争和控制权争夺的隐喻啊！生活在这种隐喻构建的观念和场景中，亲密关系从何构建？</p><h3 id="常见的个人信念税"><a href="#常见的个人信念税" class="headerlink" title="常见的个人信念税"></a>常见的个人信念税</h3><p>我们倾向于认为，大多数人持有相对正面和现实的核心信念，比如「大多数事情我能够胜任」、「我是有价值的人」，负面的核心信念只在遭遇一些应激事件导致内心痛苦时才表现出来。但只要稍微认真审视一下，或多或少会发现，一些隐蔽的信念在持续发挥着影响。即便优秀富有成效的人，也可能是受类似「我不被接受」之类的负面核心信念所驱动。</p><p>认知疗法之父 Aaron Beck 将核心信念分为三类<sup>[2]</sup>：</p><ol><li><p>无能类核心信念（Helpless Core Beliefs）</p><blockquote><p>我不能胜任<br>我做事毫无效率<br>任何事我都做不好<br>我很无助<br>我易受伤害<br>我贫苦<br>我失控<br>我是有缺陷的人 （例如，我比不上其他人）<br>我不够好 （成就方面）<br>我是失败者<br>……</p></blockquote></li></ol><ol><li><p>不可爱类核心信念（Unlovable Core Beliefs）</p><blockquote><p>我不可爱/不讨人喜欢/不受欢迎/没有吸引力<br>我是多余的<br>我被人忽视<br>我与人不同<br>我真坏（所以其他人不爱我）<br>我不够好（所以其他人不爱我）<br>我必定被拒绝<br>我必定孤独<br>……</p></blockquote></li></ol><ol><li><p>无价值类核心信念（Worthless Core Beliefs）</p><blockquote><p>我毫无价值<br>我不被接受<br>我是废物<br>我不道德<br>我很危险<br>我有罪<br>……</p></blockquote></li></ol><p>一眼看上去，大部分的核心信念好像都跟我们关系不大。</p><p>真的是这样吗？</p><p>想想，你为什么会把工作上的一件小事做得比别人好？</p><p>是真正乐于其中吗？还是害怕做不好会有后果？还是别人的称赞会让自己觉得有价值？为什么别人的称赞如此重要？为什么别人实际上没有做出的称赞成为了动力？……继续追问下去，有没有出现上面提到的核心信念？</p><p>想想，你为什么会把工作上的一件小事做得不如别人？</p><p>是不知道该怎么做吗？是觉得无聊不想做吗？是做得比别人慢吗？……继续追问下去，有没有出现上面提到的核心信念？</p><p>再想想，你为什么会在意跟别人比较工作成果？</p><p>是因为薪水吗？是因为不想被批评吗？是因为能获得成就感吗？……继续追问下去，有没有出现上面提到的核心信念？</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/beg-23.png" alt=""></p><p>也许我们从来没有追问过核心信念，所以也不知道它们会造成怎样的影响和损失。</p><p>比如，在「我不够好（成就方面）」这样一个核心信念下，会有怎样的想法和行动？可能会将某种成就作为人生准绳而忽视其他重要的东西，可能会容易迷信权威，可能不敢表达自己的观点，可能很功利，可能容易半途而废……</p><p>这些信念导致的损失，就像名目繁多的税费，看似不存在，实则难以估量，说不定就是贫富差距的罪魁祸首。</p><p>当然，还有数不清的非核心信念（假设）也在慢慢氧化我们，比如：</p><ul><li>读过的书越多，掌握的知识就越多</li><li>道理懂得越多，人的判断力越好</li><li>房子应该存够首期再买</li><li>东西越便宜越好/越贵越好</li><li>朋友多说明人缘好，人缘好说明人好</li><li>做完的东西越多越好代表效率越高</li><li>……</li></ul><p>真的真的是这样吗？我也不知道。</p><p>欢迎留言，分享你交过的信念税。</p><p>下一篇开始，我们探讨如何识别出潜在信念，找到合理避税的突破口。</p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li>[1]. <a href="https://book.douban.com/subject/26298597/" target="_blank" rel="noopener">我们赖以生存的隐喻</a></li><li>[2]. Beck, A. T., Freeman, A., &amp; Davis, D. D. (2015). Cognitive Therapy of Personality Disorders. Guilford Publications.</li><li>[3]. <a href="http://blogs.discovermagazine.com/neuroskeptic/2012/05/09/the-70000-thoughts-per-day-myth/#.WU2hihOGOHo" target="_blank" rel="noopener">The 70,000 Thoughts Per Day Myth? - Neuroskeptic</a></li><li>[4]. <a href="http://list25.com/25-famous-predictions-that-were-proven-to-be-horribly-wrong/" target="_blank" rel="noopener">25 Famous Predictions That Were Proven To Be Horribly Wrong</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/dailythoughts.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="叽歪" scheme="http://uegeek.com/categories/%E5%8F%BD%E6%AD%AA/"/>
    
    
      <category term="Psychology" scheme="http://uegeek.com/tags/Psychology/"/>
    
      <category term="HackYourself" scheme="http://uegeek.com/tags/HackYourself/"/>
    
      <category term="Mind" scheme="http://uegeek.com/tags/Mind/"/>
    
  </entry>
  
  <entry>
    <title>17° 读书会笔记[6] - 设计的准则</title>
    <link href="http://uegeek.com/170415-design-principle.html"/>
    <id>http://uegeek.com/170415-design-principle.html</id>
    <published>2017-04-15T01:03:19.000Z</published>
    <updated>2017-07-15T01:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/161218readingtitle.png" alt=""></p><a id="more"></a> <p>17° 读书会第 6 本书：<a href="https://book.douban.com/subject/26882272/" target="_blank" rel="noopener">设计的准则</a></p><p><img src="https://img3.doubanio.com/lpic/s29068132.jpg" alt=""></p><p>阅读时间：2017-03-19 ~ 2017-03-25</p><blockquote><p>这本小书不是要扼杀创造力，也不是要将其简化为一堆原则，它并非妨碍好设计发生的方程式，而是希望制止对设计专业复杂性缺乏了解的状况。大脑应该负责运用合适的方程式去实现预期的结果。</p></blockquote><h2 id="读前问题"><a href="#读前问题" class="headerlink" title="读前问题"></a>读前问题</h2><ul><li>设计中的「准则」起到什么作用？</li><li>作者认为无形因素和有形因素如何怎样影响设计？</li><li>设计的语义、句法和语用分别指什么？</li><li>哪些准则对我的启发最大？</li></ul><h2 id="读后思考"><a href="#读后思考" class="headerlink" title="读后思考"></a>读后思考</h2><p>马西莫·维涅里（Massimo Vignelli，1931—2014）是意大利国宝级设计师，现代主义设计的开拓中坚，二十世纪下半叶视觉传达发展进程中具有深远影响的人物之一。他的设计领域涵盖了包装、产品、家具、展示设计到公共标识。</p><p>1966年，维涅里同其他设计师共同创办了 Unimark International，这家设计所很快便成为了世界上最大的设计公司之一，并为众多世界知名企业设计了身份标识，如美国航空、美国布鲁明代尔百货公司、福特、IBM等。这一时期，维涅里最著名的设计作品是纽约地铁线路图。1971年，维涅里离开 Unimark，与妻子莱拉一起在纽约创办了维涅里设计事务所，被视为设计界的全能常青树。</p><p><img src="http://7xjpra.com1.z0.glb.clouddn.com/17032504bf6e7e140fe72a14841816405ec424.jpg" alt=""></p><p>这本书提供了对平面设计中「标准」的思考，有助于更好地理解版式设计的基本原理。</p><blockquote><p>设计是一种规则，是带有其自身规定法则的创造性的过程，围绕其目标，以最直接且最具有表现力的方式控制着结果的统一性。</p></blockquote><p>跟前面几本书一样，维涅里也提出了他对设计的定义。Paul Rand 认为设计是处理关系，原研哉认为设计是对生活意义的表达，Papanek 认为设计是为了达成有意义的秩序而进行的有意识而又富于直觉的努力。维涅里的定义更加带有「原教旨主义」的意味，认为设计是创造（严肃的）规则。</p><blockquote><p>规则是一组自我设定的准则，规范着我们的实践。混乱、复杂的设计揭露同样混乱、复杂的头脑。</p></blockquote><p>维涅里将设计作品中凝练的规律视为最高准绳，厌恶多余、粗鄙、杂质、混乱。他认为设计如果没有规则，将是混乱的，是一种没有责任心的活动，这就是「无规矩不成方圆」吧~</p><p>准则反应了设计师在长期的设计实践中，对「如何做出更好的设计」的种种试验和反思：哪些问题重复出现？哪些因素影响效果？哪些做法总是奏效？当设计师在「经验」和「理想的设计效果」中建立起强关联，就会将「经验」升级为「准则」。</p><p><img src="http://s3files.core77.com/blog/images/Massimo_Vignelli_Canon.jpg" alt=""></p><p>维涅里把设计的准则分成无形因素和有形因素。</p><table><thead><tr><th>无形因素</th><th>作用</th></tr></thead><tbody><tr><td>语义学</td><td>设计追寻意义。全方位多角度地理解设计主体，并且在设计主体与发送端和接收端之间，以对双方都有意义的方式建立联系。</td></tr><tr><td>句法学</td><td>在遣词造句和设计语言表达中控制语法的正确使用的准则。</td></tr><tr><td>语用学</td><td>设计所传达的意义应该被理解</td></tr><tr><td>规则</td><td>规则是一组自我设定的准则，规范着我们的实践。</td></tr><tr><td>适度性</td><td>在适当的范围内进行探索，指导我们选择正确的媒介、正确的材料、正确的比例、正确的表达方式和色彩与肌理</td></tr><tr><td>模糊性</td><td>多元，物体以不同方式真实存在的可能性</td></tr><tr><td>历史、理论与批评</td><td>从时代-运动-表现方式-设计师-设计作品中寻找「为什么」和洞察</td></tr><tr><td>现代主义</td><td>对于生产过程以及产品最终目的的认识，是一种意识形态的反应</td></tr><tr><td>设计为一</td><td>设计的规则是唯一的，可以被运用于许多不同的设计主题中，它高于并超越任何风格</td></tr><tr><td>视觉的力量</td><td>就一件设计作品而言，至关重要的是它应该充满了视觉张力，并以其独特的呈现来实现设计意图</td></tr><tr><td>智慧的优雅</td><td>引导我们走向一切创造性活动最佳解决方案的线索，是思维的终极目标</td></tr><tr><td>永恒性</td><td>探索客观价值，不追逐潮流，以恰当的方式反映内容</td></tr><tr><td>责任感</td><td>设计的结果应该能够独立存在，并且无需借口、解释、致歉</td></tr><tr><td>光</td><td>光塑造了一切「表现」</td></tr><tr><td>任意性</td><td>为了追求自由，以任意性抵制规则是错误的，而自由是有条件与界限的，反之则是混沌、任性、朝生暮死与肤浅的</td></tr><tr><td>语境</td><td>设计与被设计对象的终结目标之间的相互关系</td></tr><tr><td>影响</td><td>“影响”以深入且形成性的方式作用于思维，而“灵感”则偏向具有朝生暮死的本质</td></tr><tr><td>营销</td><td>获得市场成功需要愿景、勇气与决心，而不是市场调研与焦点族群的掣肘，好设计需要勇气</td></tr></tbody></table><p>在有形因素部分，维涅里仔细阐述了平面设计和版式设计中的一些黄金规则，包括书籍网格的使用、字体尺寸、色彩、装订等等。他极力主张简化设计元素的复杂性，通过少量的样式传达出意义、平衡、优雅。</p><blockquote><p>在人人喧嚣当中，沉默反而是会被关注的。空白空间提供了宁静——那就是我们版式设计的精髓。</p></blockquote><p>相比无形部分，有形部分的规则多少让人觉得枯燥而没有新意。正是这种近似刻板的准则，让我们感受到维涅里对设计的「混乱」充满担忧。他将「随意」导致的复杂和混乱作为设计的大敌，热爱那些大道至简的底层规律。</p><p><img src="http://monotypecom.s3.amazonaws.com/images/4ad38be7244043d8/MV_6.jpg" alt=""></p><p>对于「规则」，受限于历史、环境、载体、目的等，规则可能不再适用。但是对「什么是优雅的设计」规律的追求，所有设计师都应该永不停歇。</p><p>最后，用书中最后一段话作为结尾：</p><blockquote><p>我爱系统，而藐视偶然的巧合。</p><p>我爱模糊性，因为在我看来模糊性意味着多重的含义；我爱矛盾，因为它使事物保持发展，能避免预设某种僵化的含义，也避免停滞与压迫。</p><p>正如同我爱处于变化中的事物那样，我也乐于看它们存在于一个参照系之中——这种关照方式始终如一地保证了我自始至终是负责每一处细节的那个人。</p><p>我爱设计，这就是原因。</p></blockquote><p><img src="http://monotypecom.s3.amazonaws.com/images/972e2e2eb2dc5094/MV_1.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/161218readingtitle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="设计" scheme="http://uegeek.com/categories/%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="Design" scheme="http://uegeek.com/tags/Design/"/>
    
      <category term="设计" scheme="http://uegeek.com/tags/%E8%AE%BE%E8%AE%A1/"/>
    
      <category term="Note" scheme="http://uegeek.com/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning-2 语言模型和 N-gram</title>
    <link href="http://uegeek.com/170312-deeplearning-week1.html"/>
    <id>http://uegeek.com/170312-deeplearning-week1.html</id>
    <published>2017-03-12T13:58:23.000Z</published>
    <updated>2017-07-15T01:21:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xjpra.com1.z0.glb.clouddn.com/dl-week0-title.png" alt=""></p><a id="more"></a> <p>语言模式是自然语言处理的一个基础概念。我们可以从语料中得到「语言模型」—— 即句子的概率，可用于：</p><ul><li>发现错别句子</li><li>发现新短语</li><li>生成句子（如<a href="https://github.com/phunterlau/wangfeng-rnn" target="_blank" rel="noopener">模仿汪峰写歌</a>）</li></ul><p>机器怎样理解自然语言呢？有两种思路：</p><ul><li>学习语法：词性、句子成分，但不能保证语义，如，火星追杀绿色的梦</li><li>概率统计：<a href="https://www.wikiwand.com/zh-cn/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B" target="_blank" rel="noopener">齐夫定律</a>（词频 $\propto \frac{1}{rank}$ ：频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍），香农的信息论</li></ul><h3 id="概率论基本原理"><a href="#概率论基本原理" class="headerlink" title="概率论基本原理"></a>概率论基本原理</h3><p>概率空间：所有可能的结果。概率中的原子结构是基本事件，不可分割，不重叠；分子结构是事件（基本事件的集合）。事件的概率，可以理解为所选取的基本事件在整个空间里占的面积比例。</p><ul><li>联合概率 P(A,B)：两个事件同时发生，比如掷两次筛子，可能有 $6^2$ 种结果。</li><li>条件概率 P(B|A)：A 条件下 B 发生的概率。从一个大的空间进入到一个子空间（切片），计算在子空间中的占比。$P(B|A) = \frac{P(A,B)}{P(A)}$</li></ul><h3 id="概率语言模型"><a href="#概率语言模型" class="headerlink" title="概率语言模型"></a>概率语言模型</h3><ul><li>计算句子的概率： $P(S) = P(w_1,w_2,w_3,…,w_n)$</li><li>用处：句子错误检查、输入法候选、生成有用的句子等等</li><li>统计：随着空间膨胀，数据变稀疏，样本有效性降低</li></ul><p>对句子做最简化的处理，先考虑只有两个词的句子，根据条件概率公式，它的概率等于第一个词的空间占比，乘以第一个词的概率空间中第二个词的占比：$P(w_1,w_2) = P(w_2|w_1)*P(w_1)$</p><p>最初级的语言模型（Unigram），可以人为地假设词之间是独立的： $P(w_2|w_1) \approx P(w_2)$，于是这个句子的概率约等于两个词的频率相乘： $P(w_2,w_1) \approx P(w_1)*P(w_2)$ </p><p>如果把两个词的句子扩展为三个词：$P(w_1,w_2,w_3) = p(w_1,w_2)<em>p(w_3|w_1,w_2) = p(w_1)</em>p(w_2|w_1)*p(w_3|w_1,w_2)$</p><p>以此类推：</p><p>$P(w_1,w_2,…w<em>n) = \prod</em>{i} P(w_i|w_1w<em>2…w</em>{i-1})$</p><p>这样做的话，对每个词要考虑它前面的所有词，这在实际中意义不大。可以做些简化吗？</p><p>我们可以基于马尔科夫假设来做简化。</p><blockquote><p>马尔科夫假设是指，每个词出现的概率只跟它前面的少数几个词有关。比如，二阶马尔科夫假设只考虑前面两个词，相应的语言模型是三元模型。引入了马尔科夫假设的语言模型，也可以叫做马尔科夫模型。</p><p>马尔可夫链（Markov chain）为狀態空間中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。</p></blockquote><p>比如对上面公式做一个 i-k 的简化：</p><p>$P(w_1,w_2,…w<em>n) \approx \prod</em>{i} P(w<em>i|w</em>{i-k}…w_{i-1})$</p><p>物理意义上说，上面的公式意味着每次看到 i 时，只要关注 i 前面的 k 个词，这就是 N-gram 模型的思路。</p><h3 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h3><p>作业 1：$P(w_1,w_2) = P(w_2|w_1)*P(w_1)$ 没有减少参数个数，为什么？</p><p>作业 2：在自己选取的数据集合上建立 Bigram 模型，并使用该建立好的模型生成句子。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>技巧：进入 docker 容器的 shell 环境</p><p><code>docker exec -it container_id /bin/bash</code></p><h3 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h3><ul><li><a href="https://www.wikiwand.com/zh-cn/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B" target="_blank" rel="noopener">齐夫定律</a></li><li><a href="http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html" target="_blank" rel="noopener">蒙特卡罗方法入门 - 阮一峰的网络日志</a></li><li><a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank" rel="noopener">Language Modeling - Course notes for NLP by Michael Collins, Columbia University</a></li><li><a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank" rel="noopener">Language Modeling with Ngrams</a></li><li><a href="https://www.youtube.com/watch?v=s3kKlUBa3b0" target="_blank" rel="noopener">4 - 1 - Introduction to N-grams- Stanford NLP - Professor Dan Jurafsky &amp; Chris Manning - YouTube</a></li><li><a href="https://www.wikiwand.com/zh/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE" target="_blank" rel="noopener">马尔可夫链 - Wikiwand</a></li><li><a href="https://github.com/sunoonlee/DeepLearning101/issues/2" target="_blank" rel="noopener">sunoonlee 同学的笔记</a></li><li><a href="https://github.com/zhatrix/DeepLearning101/blob/master/ch1/project/assignmentch1.ipynb" target="_blank" rel="noopener">DeepLearning101/zhatrix/DeepLearning101</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xjpra.com1.z0.glb.clouddn.com/dl-week0-title.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="动手" scheme="http://uegeek.com/categories/%E5%8A%A8%E6%89%8B/"/>
    
    
      <category term="Deep learning" scheme="http://uegeek.com/tags/Deep-learning/"/>
    
      <category term="深度学习" scheme="http://uegeek.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://uegeek.com/tags/AI/"/>
    
      <category term="Docker" scheme="http://uegeek.com/tags/Docker/"/>
    
  </entry>
  
</feed>
